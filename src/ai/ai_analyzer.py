"""
AIåˆ†ææ¨¡å—
ä½¿ç”¨DeepSeek AIåˆ†ææ–°é—»å¯¹Aè‚¡æ¿å—çš„å½±å“
æ”¯æŒä¸²è¡Œã€å¹¶å‘å’Œå¼‚æ­¥åˆ†æ
"""

import asyncio
import json
import os
import time
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import threading
import queue

import openai
import yaml
from openai import OpenAI, AsyncOpenAI
import aiohttp
from tenacity import retry, stop_after_attempt, wait_exponential

from ..utils.database import NewsItem, db_manager
from ..utils.logger import get_logger

logger = get_logger("ai_analyzer")


@dataclass
class BatchAnalysisConfig:
    """æ‰¹é‡åˆ†æé…ç½®"""
    max_concurrent_requests: int = 10
    batch_size: int = 20
    use_async: bool = True
    use_threading: bool = True
    use_multiprocessing: bool = False
    max_workers: int = None
    retry_attempts: int = 3
    timeout_seconds: int = 300
    rate_limit_per_minute: int = 100


class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_calls_per_minute: int):
        self.max_calls = max_calls_per_minute
        self.calls = queue.Queue()
        self.lock = threading.Lock()
    
    async def acquire(self):
        """å¼‚æ­¥è·å–è®¸å¯"""
        current_time = time.time()
        
        with self.lock:
            # æ¸…ç†è¿‡æœŸçš„è°ƒç”¨è®°å½•
            while not self.calls.empty():
                call_time = self.calls.queue[0]
                if current_time - call_time > 60:
                    self.calls.get()
                else:
                    break
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if self.calls.qsize() >= self.max_calls:
                oldest_call = self.calls.queue[0]
                wait_time = 60 - (current_time - oldest_call)
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
            
            self.calls.put(current_time)


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœæ•°æ®æ¨¡å‹"""

    news_id: str
    impact_score: float  # 0åˆ°100ï¼Œæ•°å€¼è¶Šé«˜å½±å“è¶Šå¤§
    summary: str
    analysis_time: datetime

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "news_id": self.news_id,
            "impact_score": self.impact_score,
            "summary": self.summary,
            "analysis_time": self.analysis_time.isoformat(),
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "AnalysisResult":
        """ä»å­—å…¸åˆ›å»ºAnalysisResultå¯¹è±¡"""
        analysis_time = (
            datetime.fromisoformat(data["analysis_time"])
            if data.get("analysis_time")
            else datetime.now()
        )

        return cls(
            news_id=data["news_id"],
            impact_score=data["impact_score"],
            summary=data["summary"],
            analysis_time=analysis_time,
        )


class AIAnalyzer:
    """AIæ–°é—»åˆ†æå™¨ï¼Œæ”¯æŒä¸²è¡Œå’Œå¹¶å‘åˆ†æ"""

    def __init__(self, config_path: str = None, batch_config: BatchAnalysisConfig = None):
        """
        åˆå§‹åŒ–AIåˆ†æå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            batch_config: æ‰¹é‡åˆ†æé…ç½®
        """
        self.config = self._load_config(config_path)
        self.batch_config = batch_config or BatchAnalysisConfig()
        
        # å®¢æˆ·ç«¯è®¾ç½®
        self.client = None
        self.async_client = None
        self.semaphore = None
        self.rate_limiter = None
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._setup_client()
        if self.batch_config.use_async:
            self._setup_async_client()
            self._setup_concurrency_controls()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {"analyzed": 0, "errors": 0, "api_calls": 0, "total_tokens": 0}

    def _load_config(self, config_path: Optional[str]) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path is None:
            config_path = os.path.join(
                os.path.dirname(__file__), "../../config/config.yaml"
            )

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def _setup_client(self):
        """è®¾ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹DeepSeek APIï¼‰"""
        ai_config = self.config.get("ai_analysis", {}).get("deepseek", {})

        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–APIå¯†é’¥
        api_key = os.getenv("DEEPSEEK_API_KEY") or ai_config.get("api_key", "")
        if api_key.startswith("${") and api_key.endswith("}"):
            # å¤„ç†ç¯å¢ƒå˜é‡å¼•ç”¨
            env_var = api_key[2:-1]
            api_key = os.getenv(env_var, "")

        if not api_key:
            logger.warning("æœªæ‰¾åˆ°DeepSeek APIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self.client = None
            return

        try:
            self.client = OpenAI(
                api_key=api_key,
                base_url=ai_config.get("base_url", "https://api.deepseek.com/v1"),
            )
            logger.info("DeepSeek APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"DeepSeek APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None

    def _setup_async_client(self):
        """è®¾ç½®å¼‚æ­¥å®¢æˆ·ç«¯"""
        ai_config = self.config.get("ai_analysis", {}).get("deepseek", {})
        
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–APIå¯†é’¥
        api_key = os.getenv("DEEPSEEK_API_KEY") or ai_config.get("api_key", "")
        if api_key.startswith("${") and api_key.endswith("}"):
            # å¤„ç†ç¯å¢ƒå˜é‡å¼•ç”¨
            env_var = api_key[2:-1]
            api_key = os.getenv(env_var, "")

        if api_key:
            try:
                self.async_client = AsyncOpenAI(
                    api_key=api_key,
                    base_url=ai_config.get("base_url", "https://api.deepseek.com/v1"),
                )
                logger.info("å¼‚æ­¥DeepSeek APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"å¼‚æ­¥DeepSeek APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.async_client = None

    def _setup_concurrency_controls(self):
        """è®¾ç½®å¹¶å‘æ§åˆ¶"""
        self.semaphore = asyncio.Semaphore(self.batch_config.max_concurrent_requests)
        self.rate_limiter = RateLimiter(self.batch_config.rate_limit_per_minute)

    def analyze_news(self, news_item: NewsItem) -> AnalysisResult:
        """
        åˆ†æå•æ¡æ–°é—»

        Args:
            news_item: æ–°é—»é¡¹

        Returns:
            AnalysisResult: åˆ†æç»“æœ
        """
        try:
            if not self.client:
                return self._mock_analysis(news_item)

            # æ„å»ºæç¤ºè¯
            prompt = self._build_analysis_prompt(news_item)

            # é¦–å…ˆå°è¯•ä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆæ€è€ƒæ¨¡å‹ï¼‰
            try:
                response = self._call_deepseek_api(prompt)
                result = self._parse_analysis_response(news_item.id, response)
                self.stats["analyzed"] += 1
                logger.debug(f"æ–°é—»åˆ†æå®Œæˆï¼ˆä¸»æ¨¡å‹ï¼‰: {news_item.title[:50]}...")
                return result
                
            except Exception as primary_error:
                logger.warning(f"ä¸»æ¨¡å‹åˆ†æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹: {primary_error}")
                
                # ä½¿ç”¨å¤‡ç”¨æ¨¡å‹é‡è¯•
                response = self._call_deepseek_api_fallback(prompt)
                result = self._parse_analysis_response(news_item.id, response)
                self.stats["analyzed"] += 1
                logger.debug(f"æ–°é—»åˆ†æå®Œæˆï¼ˆå¤‡ç”¨æ¨¡å‹ï¼‰: {news_item.title[:50]}...")
                return result

        except Exception as e:
            logger.error(f"æ–°é—»åˆ†æå¤±è´¥: {e}")
            self.stats["errors"] += 1
            return self._error_fallback_analysis(news_item)

    def analyze_single_news(self, news_item: NewsItem) -> AnalysisResult:
        """
        åˆ†æå•æ¡æ–°é—»ï¼ˆåˆ«åæ–¹æ³•ï¼Œå…¼å®¹æµ‹è¯•ï¼‰

        Args:
            news_item: æ–°é—»é¡¹

        Returns:
            AnalysisResult: åˆ†æç»“æœ
        """
        return self.analyze_news(news_item)

    def batch_analyze(self, news_list: List[NewsItem]) -> List[AnalysisResult]:
        """
        æ‰¹é‡åˆ†ææ–°é—»

        Args:
            news_list: æ–°é—»åˆ—è¡¨

        Returns:
            List[AnalysisResult]: åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        batch_size = (
            self.config.get("ai_analysis", {})
            .get("analysis_params", {})
            .get("batch_size", 10)
        )

        logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(news_list)} æ¡æ–°é—»ï¼Œæ‰¹é‡å¤§å°: {batch_size}")

        for i in range(0, len(news_list), batch_size):
            batch = news_list[i : i + batch_size]

            for news_item in batch:
                try:
                    result = self.analyze_news(news_item)
                    results.append(result)

                    # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
                    self._save_analysis_result(result)

                    # é¿å…APIé™æµ
                    time.sleep(0.1)

                except Exception as e:
                    logger.error(f"æ‰¹é‡åˆ†æä¸­å•é¡¹å¤±è´¥: {e}")
                    continue

            # æ‰¹æ¬¡é—´æš‚åœ
            if i + batch_size < len(news_list):
                time.sleep(1)

        logger.info(
            f"æ‰¹é‡åˆ†æå®Œæˆ: æˆåŠŸ {len(results)} æ¡ï¼Œå¤±è´¥ {len(news_list) - len(results)} æ¡"
        )
        return results

    def _build_analysis_prompt(self, news_item: NewsItem) -> str:
        """
        æ„å»ºåˆ†ææç¤ºè¯

        Args:
            news_item: æ–°é—»é¡¹

        Returns:
            str: æç¤ºè¯
        """
        prompt = f"""
è¯·ä½ ä½œä¸ºä¸€ä½ä¸“ä¸šçš„Aè‚¡å¸‚åœºåˆ†æå¸ˆï¼Œå¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œæ·±åº¦åˆ†æï¼Œé‡ç‚¹å…³æ³¨å…¶å¯¹Aè‚¡å¸‚åœºçš„å½±å“ã€‚

æ–°é—»ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{news_item.title}
å†…å®¹ï¼š{news_item.content}
æ¥æºï¼š{news_item.source}
å‘å¸ƒæ—¶é—´ï¼š{news_item.publish_time}
å…³é”®è¯ï¼š{', '.join(news_item.keywords)}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
    "impact_score": æ•°å€¼(0åˆ°100ï¼Œæ•°å€¼è¶Šé«˜å½±å“è¶Šå¤§),
    "summary": "æ–°é—»å½±å“æ‘˜è¦(100å­—ä»¥å†…)"
}}

åˆ†æè¦æ±‚ï¼š
1. å½±å“è¯„åˆ†èŒƒå›´ï¼š0ï¼ˆæ— å½±å“ï¼‰åˆ° 100ï¼ˆæåº¦æ­£é¢ï¼‰ï¼Œæ•°å€¼è¶Šé«˜å½±å“è¶Šå¤§
2. æŠ•èµ„å»ºè®®è¦å…·ä½“ã€å¯æ“ä½œï¼Œé¿å…æ¨¡ç³Šè¡¨è¿°

è¯·ç¡®ä¿è¾“å‡ºä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ã€‚
"""
        return prompt.strip()

    def _call_deepseek_api(self, prompt: str) -> str:
        """
        è°ƒç”¨DeepSeek API

        Args:
            prompt: æç¤ºè¯

        Returns:
            str: APIå“åº”å†…å®¹
        """
        ai_config = self.config.get("ai_analysis", {}).get("deepseek", {})
        analysis_params = self.config.get("ai_analysis", {}).get("analysis_params", {})

        # è®°å½•APIè¯·æ±‚è¯¦æƒ…
        model = ai_config.get("model", "deepseek-chat")
        max_tokens = ai_config.get("max_tokens", 2000)
        temperature = ai_config.get("temperature", 0.1)
        timeout = analysis_params.get("timeout", 30)
        base_url = ai_config.get("base_url", "https://api.deepseek.com/v1")
        
        logger.info(f"ğŸ”„ å‡†å¤‡è°ƒç”¨DeepSeek API")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   åŸºç¡€URL: {base_url}")
        logger.info(f"   æœ€å¤§ä»¤ç‰Œ: {max_tokens}")
        logger.info(f"   æ¸©åº¦: {temperature}")
        logger.info(f"   è¶…æ—¶: {timeout}ç§’")
        logger.info(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

        try:
            import time
            start_time = time.time()
            
            logger.info(f"ğŸ“¤ å¼€å§‹APIè¯·æ±‚...")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Aè‚¡å¸‚åœºåˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„è‚¡ç¥¨æŠ•èµ„ç»éªŒå’Œæ·±åšçš„å¸‚åœºæ´å¯ŸåŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=timeout,
            )

            end_time = time.time()
            response_time = end_time - start_time
            
            logger.info(f"ğŸ“¥ APIå“åº”æˆåŠŸ")
            logger.info(f"   å“åº”æ—¶é—´: {response_time:.2f}ç§’")

            self.stats["api_calls"] += 1
            if hasattr(response, "usage") and response.usage:
                self.stats["total_tokens"] += response.usage.total_tokens
                logger.info(f"   ä½¿ç”¨ä»¤ç‰Œ: {response.usage.total_tokens}")
                logger.info(f"   è¾“å…¥ä»¤ç‰Œ: {response.usage.prompt_tokens}")
                logger.info(f"   è¾“å‡ºä»¤ç‰Œ: {response.usage.completion_tokens}")

            response_content = response.choices[0].message.content
            logger.info(f"   å“åº”å†…å®¹é•¿åº¦: {len(response_content)} å­—ç¬¦")
            logger.debug(f"   å“åº”å†…å®¹é¢„è§ˆ: {response_content[:200]}...")
            
            # è®°å½•å®Œæ•´å“åº”å†…å®¹ç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“„ å®Œæ•´APIå“åº”å†…å®¹:")
            logger.info(f"   {response_content}")

            return response_content

        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            
            logger.error(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"   å¤±è´¥æ—¶é—´: {response_time:.2f}ç§’")
            
            # è®°å½•æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if hasattr(e, 'response'):
                logger.error(f"   HTTPçŠ¶æ€ç : {getattr(e.response, 'status_code', 'N/A')}")
                logger.error(f"   å“åº”å¤´: {getattr(e.response, 'headers', 'N/A')}")
                try:
                    logger.error(f"   å“åº”å†…å®¹: {e.response.text[:500]}...")
                except:
                    logger.error(f"   æ— æ³•è¯»å–å“åº”å†…å®¹")
            
            if hasattr(e, 'code'):
                logger.error(f"   é”™è¯¯ä»£ç : {e.code}")
                
            raise

    def _call_deepseek_api_fallback(self, prompt: str) -> str:
        """
        è°ƒç”¨DeepSeek APIï¼ˆå¤‡ç”¨æ¨¡å‹ï¼‰

        Args:
            prompt: æç¤ºè¯

        Returns:
            str: APIå“åº”å†…å®¹
        """
        ai_config = self.config.get("ai_analysis", {}).get("deepseek", {})
        analysis_params = self.config.get("ai_analysis", {}).get("analysis_params", {})

        # ä½¿ç”¨å¤‡ç”¨æ¨¡å‹é…ç½®
        fallback_model = ai_config.get("fallback_model", "deepseek-chat")
        fallback_timeout = analysis_params.get("fallback_timeout", 30)
        
        logger.info(f"ğŸ”„ å‡†å¤‡è°ƒç”¨DeepSeek APIï¼ˆå¤‡ç”¨æ¨¡å‹ï¼‰")
        logger.info(f"   å¤‡ç”¨æ¨¡å‹: {fallback_model}")
        logger.info(f"   å¤‡ç”¨è¶…æ—¶: {fallback_timeout}ç§’")

        try:
            import time
            start_time = time.time()
            
            logger.info(f"ğŸ“¤ å¼€å§‹å¤‡ç”¨APIè¯·æ±‚...")
            
            response = self.client.chat.completions.create(
                model=fallback_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Aè‚¡å¸‚åœºåˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„è‚¡ç¥¨æŠ•èµ„ç»éªŒå’Œæ·±åšçš„å¸‚åœºæ´å¯ŸåŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=ai_config.get("max_tokens", 2000),
                temperature=ai_config.get("temperature", 0.1),
                timeout=fallback_timeout,
            )

            end_time = time.time()
            response_time = end_time - start_time
            
            logger.info(f"ğŸ“¥ å¤‡ç”¨APIå“åº”æˆåŠŸ")
            logger.info(f"   å“åº”æ—¶é—´: {response_time:.2f}ç§’")

            response_content = response.choices[0].message.content
            logger.info(f"   å“åº”å†…å®¹é•¿åº¦: {len(response_content)} å­—ç¬¦")
            
            # è®°å½•å®Œæ•´å“åº”å†…å®¹ç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“„ å¤‡ç”¨APIå®Œæ•´å“åº”å†…å®¹:")
            logger.info(f"   {response_content}")

            return response_content

        except Exception as e:
            logger.error(f"âŒ å¤‡ç”¨DeepSeek APIè°ƒç”¨ä¹Ÿå¤±è´¥: {e}")
            raise

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def _async_analyze_news(self, news_item: NewsItem) -> AnalysisResult:
        """å¼‚æ­¥åˆ†æå•æ¡æ–°é—»"""
        if not self.semaphore:
            self.semaphore = asyncio.Semaphore(self.batch_config.max_concurrent_requests)
        if not self.rate_limiter:
            self.rate_limiter = RateLimiter(self.batch_config.rate_limit_per_minute)
            
        logger.info(f"ğŸ”„ å¼€å§‹å¼‚æ­¥åˆ†ææ–°é—»: {news_item.title[:30]}...")
        logger.info(f"   ä¿¡å·é‡çŠ¶æ€: {self.semaphore._value}/{self.batch_config.max_concurrent_requests}")
        
        async with self.semaphore:
            logger.info(f"ğŸ“¥ è·å–åˆ°ä¿¡å·é‡ï¼Œå¼€å§‹åˆ†æ: {news_item.title[:30]}...")
            
            try:
                await self.rate_limiter.acquire()
                logger.info(f"ğŸ“Š é€šè¿‡é€Ÿç‡é™åˆ¶æ£€æŸ¥")
                
                if not self.async_client:
                    logger.warning(f"â— å¼‚æ­¥å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
                    return self._mock_analysis(news_item)
                
                prompt = self._build_analysis_prompt(news_item)
                
                logger.info(f"ğŸš€ å‘èµ·APIè¯·æ±‚ (æ¨¡å‹: {self.config.get('ai_analysis', {}).get('deepseek', {}).get('model', 'deepseek-chat')})")
                start_time = time.time()
                
                response = await self.async_client.chat.completions.create(
                    model=self.config.get("ai_analysis", {}).get("deepseek", {}).get("model", "deepseek-chat"),
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Aè‚¡å¸‚åœºåˆ†æå¸ˆã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚",
                        },
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=self.config.get("ai_analysis", {}).get("deepseek", {}).get("max_tokens", 2000),
                    temperature=self.config.get("ai_analysis", {}).get("deepseek", {}).get("temperature", 0.1),
                    timeout=self.batch_config.timeout_seconds,
                )
                
                end_time = time.time()
                response_time = end_time - start_time
                
                logger.info(f"âœ… APIå“åº”æˆåŠŸ: {news_item.title[:30]}...")
                logger.info(f"   å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                
                if hasattr(response, "usage") and response.usage:
                    logger.info(f"   Tokenä½¿ç”¨: {response.usage.total_tokens}")
                
                result = self._parse_analysis_response(
                    news_item.id, 
                    response.choices[0].message.content
                )
                
                self.stats["analyzed"] += 1
                logger.info(f"ğŸ¯ å¼‚æ­¥åˆ†æå®Œæˆ: {news_item.title[:30]}... (å½±å“è¯„åˆ†: {result.impact_score})")
                return result
                
            except asyncio.TimeoutError as e:
                logger.error(f"â° å¼‚æ­¥åˆ†æè¶…æ—¶: {news_item.title[:30]}... (è¶…æ—¶æ—¶é—´: {self.batch_config.timeout_seconds}ç§’)")
                self.stats["errors"] += 1
                return self._error_fallback_analysis(news_item)
            except Exception as e:
                error_type = type(e).__name__
                logger.error(f"âŒ å¼‚æ­¥åˆ†æå¤±è´¥: {news_item.title[:30]}...")
                logger.error(f"   é”™è¯¯ç±»å‹: {error_type}")
                logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
                
                # ç‰¹æ®Šå¤„ç†APIé™æµé”™è¯¯
                if "429" in str(e) or "rate" in str(e).lower():
                    logger.warning(f"ğŸš¦ é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå»¶è¿Ÿåé‡è¯•")
                    await asyncio.sleep(2)
                elif "503" in str(e) or "server" in str(e).lower():
                    logger.warning(f"ğŸ”§ æœåŠ¡å™¨ç¹å¿™ï¼Œå»¶è¿Ÿåé‡è¯•")
                    await asyncio.sleep(5)
                
                self.stats["errors"] += 1
                return self._error_fallback_analysis(news_item)

    async def async_batch_analyze(self, news_list: List[NewsItem]) -> List[AnalysisResult]:
        """å¼‚æ­¥æ‰¹é‡åˆ†ææ–°é—»"""
        if not news_list:
            return []
        
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ‰¹é‡åˆ†æ {len(news_list)} æ¡æ–°é—»")
        logger.info(f"   å¹¶å‘è®¾ç½®: {self.batch_config.max_concurrent_requests} ä¸ªå¹¶å‘è¯·æ±‚")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {self.batch_config.batch_size}")
        logger.info(f"   è¶…æ—¶è®¾ç½®: {self.batch_config.timeout_seconds} ç§’")
        logger.info(f"   é€Ÿç‡é™åˆ¶: {self.batch_config.rate_limit_per_minute} è¯·æ±‚/åˆ†é’Ÿ")
        
        start_time = time.time()
        
        # é‡æ–°è®¾ç½®ä¿¡å·é‡
        if not self.semaphore:
            self.semaphore = asyncio.Semaphore(self.batch_config.max_concurrent_requests)
            logger.info(f"ğŸ”§ åˆ›å»ºæ–°çš„ä¿¡å·é‡: {self.batch_config.max_concurrent_requests}")
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        logger.info(f"ğŸ“‹ åˆ›å»º {len(news_list)} ä¸ªå¼‚æ­¥ä»»åŠ¡...")
        tasks = [self._async_analyze_news(news_item) for news_item in news_list]
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…åˆ›å»ºè¿‡å¤šä»»åŠ¡
        results = []
        batch_size = self.batch_config.batch_size
        total_batches = (len(tasks) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ“¦ åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} ä¸ªä»»åŠ¡")
        
        for batch_idx in range(0, len(tasks), batch_size):
            batch_tasks = tasks[batch_idx:batch_idx + batch_size]
            current_batch = (batch_idx // batch_size) + 1
            
            logger.info(f"âš¡ å¤„ç†ç¬¬ {current_batch}/{total_batches} æ‰¹æ¬¡ ({len(batch_tasks)} ä¸ªä»»åŠ¡)")
            batch_start_time = time.time()
            
            try:
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                batch_end_time = time.time()
                batch_duration = batch_end_time - batch_start_time
                logger.info(f"âœ… æ‰¹æ¬¡ {current_batch} å®Œæˆï¼Œè€—æ—¶: {batch_duration:.2f} ç§’")
                
                # å¤„ç†å¼‚å¸¸ç»“æœ
                success_count = 0
                error_count = 0
                
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        error_count += 1
                        logger.error(f"âŒ æ‰¹æ¬¡ {current_batch} ä»»åŠ¡ {j+1} å¤±è´¥: {result}")
                        # åˆ›å»ºé”™è¯¯åå¤‡ç»“æœ
                        news_item = news_list[batch_idx + j]
                        result = self._error_fallback_analysis(news_item)
                    else:
                        success_count += 1
                    
                    results.append(result)
                
                logger.info(f"ğŸ“Š æ‰¹æ¬¡ {current_batch} ç»Ÿè®¡: æˆåŠŸ {success_count}, å¤±è´¥ {error_count}")
                
                # é¿å…APIé€Ÿç‡é™åˆ¶
                if current_batch < total_batches:
                    sleep_time = 0.5
                    logger.info(f"ğŸ˜´ æ‰¹æ¬¡é—´æš‚åœ {sleep_time} ç§’...")
                    await asyncio.sleep(sleep_time)
                    
            except Exception as e:
                logger.error(f"ğŸ’¥ æ‰¹æ¬¡ {current_batch} æ•´ä½“å¤±è´¥: {e}")
                # ä¸ºæ•´ä¸ªæ‰¹æ¬¡åˆ›å»ºé”™è¯¯ç»“æœ
                for j in range(len(batch_tasks)):
                    news_item = news_list[batch_idx + j]
                    results.append(self._error_fallback_analysis(news_item))
        
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info(f"ğŸ¯ å¼‚æ­¥æ‰¹é‡åˆ†æå®Œæˆ!")
        logger.info(f"   æ€»ç»“æœ: {len(results)} æ¡")
        logger.info(f"   æ€»è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"   å¹³å‡è€—æ—¶: {duration/len(results):.2f} ç§’/æ¡")
        logger.info(f"   æˆåŠŸç‡: {self.stats['analyzed']}/{len(results)} ({self.stats['analyzed']/len(results)*100:.1f}%)")
        
        return results

    def enhanced_batch_analyze(self, news_list: List[NewsItem]) -> List[AnalysisResult]:
        """å¢å¼ºç‰ˆæ‰¹é‡åˆ†æ - å¼ºåˆ¶ä½¿ç”¨å¹¶å‘ç­–ç•¥"""
        if not news_list:
            return []
        
        news_count = len(news_list)
        logger.info(f"å¼€å§‹å¹¶å‘æ‰¹é‡åˆ†æ {news_count} æ¡æ–°é—»")
        
        # åªè¦æ–°é—»æ•°é‡å¤§äº2å°±ä½¿ç”¨å¹¶å‘
        if news_count > 2:
            logger.info("âœ… ä½¿ç”¨å¼‚æ­¥å¹¶å‘ç­–ç•¥")
            try:
                # ç¡®ä¿å¼‚æ­¥å®¢æˆ·ç«¯å’Œå¹¶å‘æ§åˆ¶å·²åˆå§‹åŒ–
                if not self.async_client:
                    logger.info("åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯...")
                    self._setup_async_client()
                    
                if not self.semaphore or not self.rate_limiter:
                    logger.info("åˆå§‹åŒ–å¹¶å‘æ§åˆ¶...")
                    self._setup_concurrency_controls()
                
                if not self.async_client:
                    logger.warning("å¼‚æ­¥å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
                    return [self._mock_analysis(news) for news in news_list]
                
                return asyncio.run(self.async_batch_analyze(news_list))
            except Exception as e:
                logger.error(f"å¼‚æ­¥åˆ†æå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # å¤±è´¥æ—¶ä½¿ç”¨æ¨¡æ‹Ÿåˆ†æè€Œä¸æ˜¯ä¸²è¡Œåˆ†æ
                return [self._mock_analysis(news) for news in news_list]
        else:
            logger.info("ğŸ“Š æ–°é—»æ•°é‡â‰¤2ï¼Œä½¿ç”¨å•æ¡åˆ†æ")
            return [self.analyze_news(news) for news in news_list]

    def _parse_analysis_response(self, news_id: str, response: str) -> AnalysisResult:
        """
        è§£æAPIå“åº”

        Args:
            news_id: æ–°é—»ID
            response: APIå“åº”å†…å®¹

        Returns:
            AnalysisResult: è§£æåçš„åˆ†æç»“æœ
        """
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            response = response.strip()

            # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
            start_idx = response.find("{")
            end_idx = response.rfind("}") + 1

            if start_idx == -1 or end_idx == 0:
                raise ValueError("å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")

            json_str = response[start_idx:end_idx]
            data = json.loads(json_str)

            # éªŒè¯å’Œè§„èŒƒåŒ–æ•°æ®
            impact_score = float(data.get("impact_score", 0))
            impact_score = max(0, min(100, impact_score))  # é™åˆ¶èŒƒå›´

            return AnalysisResult(
                news_id=news_id,
                impact_score=impact_score,
                summary=data.get("summary", ""),
                analysis_time=datetime.now(),
            )

        except (json.JSONDecodeError, ValueError, KeyError) as e:
            logger.error(f"è§£æAPIå“åº”å¤±è´¥: {e}, å“åº”å†…å®¹: {response[:200]}...")
            return self._create_fallback_result(news_id, response)

    def _create_fallback_result(self, news_id: str, response: str) -> AnalysisResult:
        """
        åˆ›å»ºåå¤‡åˆ†æç»“æœ

        Args:
            news_id: æ–°é—»ID
            response: åŸå§‹å“åº”

        Returns:
            AnalysisResult: åå¤‡åˆ†æç»“æœ
        """
        return AnalysisResult(
            news_id=news_id,
            impact_score=0,
            summary="AIè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æç»“æœ",
            analysis_time=datetime.now(),
        )

    def _mock_analysis(self, news_item: NewsItem) -> AnalysisResult:
        """
        æ¨¡æ‹Ÿåˆ†æï¼ˆå½“APIä¸å¯ç”¨æ—¶ï¼‰

        Args:
            news_item: æ–°é—»é¡¹

        Returns:
            AnalysisResult: æ¨¡æ‹Ÿçš„åˆ†æç»“æœ
        """
        # åŸºäºå…³é”®è¯çš„ç®€å•è§„åˆ™åˆ†æ
        title_content = f"{news_item.title} {news_item.content}".lower()

        # æ¿å—æ˜ å°„
        sector_keywords = {
            "ç§‘æŠ€": ["ai", "äººå·¥æ™ºèƒ½", "èŠ¯ç‰‡", "åŠå¯¼ä½“", "äº’è”ç½‘", "äº‘è®¡ç®—", "å¤§æ•°æ®"],
            "é‡‘è": ["é“¶è¡Œ", "ä¿é™©", "è¯åˆ¸", "åŸºé‡‘", "é‡‘è", "è´·æ¬¾"],
            "åŒ»è¯": ["åŒ»è¯", "ç”Ÿç‰©", "ç–«è‹—", "åŒ»ç–—", "å¥åº·", "åˆ¶è¯"],
            "æ–°èƒ½æº": ["æ–°èƒ½æº", "é”‚ç”µæ± ", "å…‰ä¼", "é£ç”µ", "ç”µåŠ¨è½¦", "å……ç”µæ¡©"],
            "æ¶ˆè´¹": ["æ¶ˆè´¹", "é›¶å”®", "ç”µå•†", "é£Ÿå“", "é¥®æ–™", "æœè£…"],
            "åœ°äº§": ["æˆ¿åœ°äº§", "å»ºç­‘", "åœ°äº§", "æ¥¼å¸‚", "ä½æˆ¿"],
        }

        # ç®€å•æƒ…æ„Ÿåˆ†æ
        positive_words = ["ä¸Šæ¶¨", "å¢é•¿", "åˆ©å¥½", "çªç ´", "åˆ›æ–°", "æˆåŠŸ", "ç›ˆåˆ©"]
        negative_words = ["ä¸‹è·Œ", "äºæŸ", "é£é™©", "å±æœº", "å¤±è´¥", "æš´è·Œ", "é—®é¢˜"]

        positive_score = sum(1 for word in positive_words if word in title_content)
        negative_score = sum(1 for word in negative_words if word in title_content)

        if positive_score > negative_score:
            impact_score = min(positive_score * 15, 80)  # è°ƒæ•´åˆ°0-100èŒƒå›´
        elif negative_score > positive_score:
            impact_score = max(100 - negative_score * 15, 20)  # è´Ÿé¢å½±å“ç”¨è¾ƒä½åˆ†æ•°è¡¨ç¤º
        else:
            impact_score = 50  # ä¸­æ€§å½±å“

        return AnalysisResult(
            news_id=news_item.id,
            impact_score=impact_score,
            summary=f"åŸºäºå…³é”®è¯åˆ†æï¼Œæ–°é—»å¯¹Aè‚¡å¸‚åœºæœ‰{impact_score:.0f}åˆ†çš„å½±å“",
            analysis_time=datetime.now(),
        )

    def _error_fallback_analysis(self, news_item: NewsItem) -> AnalysisResult:
        """
        é”™è¯¯æ—¶çš„åå¤‡åˆ†æ

        Args:
            news_item: æ–°é—»é¡¹

        Returns:
            AnalysisResult: åå¤‡åˆ†æç»“æœ
        """
        return AnalysisResult(
            news_id=news_item.id,
            impact_score=0,
            summary="åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆåˆ†æ",
            analysis_time=datetime.now(),
        )

    def _save_analysis_result(self, result: AnalysisResult) -> bool:
        """
        ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“

        Args:
            result: åˆ†æç»“æœ

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            import sqlite3

            db_path = (
                self.config.get("database", {})
                .get("sqlite", {})
                .get("db_path", "data/news.db")
            )

            with sqlite3.connect(db_path) as conn:
                cursor = conn.cursor()

                data = result.to_dict()
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO analysis_results 
                    (news_id, impact_score, summary, analysis_time)
                    VALUES (?, ?, ?, ?)
                """,
                    (
                        data["news_id"],
                        data["impact_score"],
                        data["summary"],
                        data["analysis_time"],
                    ),
                )

                conn.commit()
                return True

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return False

    def format_analysis_report(self, results: List[AnalysisResult]) -> str:
        """
        æ ¼å¼åŒ–åˆ†ææŠ¥å‘Š

        Args:
            results: åˆ†æç»“æœåˆ—è¡¨

        Returns:
            str: æ ¼å¼åŒ–çš„æŠ¥å‘Š
        """
        if not results:
            return "æš‚æ— åˆ†æç»“æœ"

        report = f"""
# Aè‚¡æ–°é—»å½±å“åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ–°é—»æ•°é‡**: {len(results)}

## æ•´ä½“æ¦‚å†µ

"""

        # ç»Ÿè®¡ä¿¡æ¯
        positive_count = sum(1 for r in results if r.impact_score > 0) # å½±å“ä¸ºæ­£çš„æ–°é—»
        negative_count = sum(1 for r in results if r.impact_score < 0) # å½±å“ä¸ºè´Ÿçš„æ–°é—»
        neutral_count = sum(1 for r in results if r.impact_score == 0) # å½±å“ä¸º0çš„æ–°é—»

        report += f"""
- **æ­£é¢æ–°é—»**: {positive_count} æ¡
- **è´Ÿé¢æ–°é—»**: {negative_count} æ¡  
- **ä¸­æ€§æ–°é—»**: {neutral_count} æ¡

"""

        # æ¿å—å½±å“ç»Ÿè®¡
        sector_impact = {}
        # æ¿å—å½±å“åˆ†æå·²åˆ é™¤
        
        if False:  # æ¿å—å½±å“åˆ†æå·²åˆ é™¤ï¼Œè·³è¿‡æ­¤éƒ¨åˆ†
            report += "## æ¿å—å½±å“æ’å\n\n"

        # é‡è¦åˆ†æç»“æœ
        high_impact_results = [r for r in results if r.impact_score > 50] # é«˜å½±å“æ–°é—»
        if high_impact_results:
            report += f"\n## é«˜å½±å“æ–°é—»åˆ†æ ({len(high_impact_results)}æ¡)\n\n"

            for i, result in enumerate(high_impact_results[:5], 1):
                report += f"""
### {i}. å½±å“è¯„åˆ†: {result.impact_score:.1f} | å½±å“çº§åˆ«: é«˜

**æ–°é—»æ‘˜è¦**: {result.summary}

---

"""

        return report

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–åˆ†æå™¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            **self.stats,
            "last_analysis_time": datetime.now().isoformat(),
            "api_available": self.client is not None,
        }


def analyze_latest_news(limit: int = 20) -> List[AnalysisResult]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ†ææœ€æ–°æ–°é—»

    Args:
        limit: åˆ†æçš„æ–°é—»æ•°é‡é™åˆ¶

    Returns:
        List[AnalysisResult]: åˆ†æç»“æœåˆ—è¡¨
    """
    analyzer = AIAnalyzer()
    news_list = db_manager.get_news_items(limit=limit)

    if not news_list:
        logger.info("æ²¡æœ‰æ‰¾åˆ°å¾…åˆ†æçš„æ–°é—»")
        return []

    return analyzer.batch_analyze(news_list)


def create_enhanced_analyzer(
    max_concurrent: int = 10,
    use_async: bool = True,
    timeout_seconds: int = 300,
    rate_limit: int = 100
) -> AIAnalyzer:
    """åˆ›å»ºå¢å¼ºç‰ˆåˆ†æå™¨çš„ä¾¿æ·å‡½æ•°ï¼Œé»˜è®¤å¯ç”¨å¹¶å‘"""
    config = BatchAnalysisConfig(
        max_concurrent_requests=max_concurrent,
        use_async=use_async,
        timeout_seconds=timeout_seconds,
        rate_limit_per_minute=rate_limit
    )
    return AIAnalyzer(batch_config=config)


if __name__ == "__main__":
    # æµ‹è¯•åˆ†æåŠŸèƒ½
    results = analyze_latest_news(5)
    print(f"åˆ†æäº† {len(results)} æ¡æ–°é—»")

    if results:
        analyzer = AIAnalyzer()
        report = analyzer.format_analysis_report(results)
        print(report)
