#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»æ·±åº¦åˆ†æå™¨
é’ˆå¯¹é«˜é‡è¦æ€§æ–°é—»ï¼ˆ>70åˆ†ï¼‰è¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. ç™¾åº¦æœç´¢è·å–èƒŒæ™¯ä¿¡æ¯
2. æ·±åº¦åˆ†ææŠ¥å‘Šç”Ÿæˆ
3. é‡è¦æ€§åˆ†æ•°é‡æ–°è¯„ä¼°
"""

import json
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Union, Tuple
import re

from openai import OpenAI
import yaml

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    from src.utils.database import NewsItem
    from src.utils.logger import get_logger
    from src.ai.ai_tools.baidu_search import baidu_search_tool
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å—ï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from ..utils.database import NewsItem
    from ..utils.logger import get_logger
    from ..ai_tools.baidu_search import baidu_search_tool

logger = get_logger("deep_analyzer")


@dataclass
class DeepAnalysisResult:
    """æ·±åº¦åˆ†æç»“æœ"""
    news_id: str
    title: str
    original_score: int           # åŸå§‹é‡è¦æ€§åˆ†æ•°
    search_keywords: List[str]    # æœç´¢å…³é”®è¯
    search_results_summary: str   # æœç´¢ç»“æœæ‘˜è¦
    deep_analysis_report: str     # 200å­—æ·±åº¦åˆ†ææŠ¥å‘Š
    adjusted_score: int           # è°ƒæ•´åçš„é‡è¦æ€§åˆ†æ•°
    analysis_time: str
    search_success: bool          # æœç´¢æ˜¯å¦æˆåŠŸ
    model_used: str              # ä½¿ç”¨çš„AIæ¨¡å‹


class DeepAnalyzer:
    """æ–°é—»æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ–æ·±åº¦åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or self._load_config()
        self.client = None
        self._init_client()
        
        # æ·±åº¦åˆ†æé…ç½®
        self.deep_config = self.config.get("ai_analysis", {}).get("deep_analysis", {})
        self.enabled = self.deep_config.get("enabled", True)
        self.score_threshold = self.deep_config.get("score_threshold", 70)
        self.max_concurrent = self.deep_config.get("max_concurrent", 3)
        self.search_timeout = self.deep_config.get("search_timeout", 30)
        self.max_keywords = self.deep_config.get("max_search_keywords", 5)
        self.report_max_length = self.deep_config.get("report_max_length", 200)
        self.enable_score_adjustment = self.deep_config.get("enable_score_adjustment", True)
        self.search_retry_count = self.deep_config.get("search_retry_count", 2)
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = os.path.join(
            os.path.dirname(__file__), "../../config/config.yaml"
        )
        
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯"""
        try:
            openrouter_config = self.config.get("ai_analysis", {}).get("openrouter", {})
            
            if not openrouter_config.get("api_key"):
                logger.warning("æœªé…ç½®OpenRouter APIå¯†é’¥ï¼Œæ·±åº¦åˆ†æå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.client = None
                return
            
            self.client = OpenAI(
                api_key=openrouter_config.get("api_key"),
                base_url=openrouter_config.get("base_url", "https://openrouter.ai/api/v1")
            )
            
            logger.info("OpenRouteræ·±åº¦åˆ†æAPIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯å¤±è´¥: {e}")
            self.client = None
    
    def should_analyze(self, news_item: NewsItem) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ·±åº¦åˆ†æ
        
        Args:
            news_item: æ–°é—»é¡¹
            
        Returns:
            bool: æ˜¯å¦éœ€è¦æ·±åº¦åˆ†æ
        """
        if not self.enabled:
            return False
            
        if not hasattr(news_item, 'importance_score') or news_item.importance_score is None:
            return False
            
        return news_item.importance_score >= self.score_threshold
    
    def analyze_news_deep(self, news_item: NewsItem) -> DeepAnalysisResult:
        """
        å¯¹å•æ¡æ–°é—»è¿›è¡Œæ·±åº¦åˆ†æ
        
        Args:
            news_item: æ–°é—»é¡¹
            
        Returns:
            DeepAnalysisResult: æ·±åº¦åˆ†æç»“æœ
        """
        if not self.should_analyze(news_item):
            logger.debug(f"æ–°é—»ä¸æ»¡è¶³æ·±åº¦åˆ†ææ¡ä»¶: {news_item.title}")
            return self._create_skip_result(news_item)
        
        try:
            logger.info(f"å¼€å§‹æ·±åº¦åˆ†æ: {news_item.title[:50]}...")
            
            # 1. æå–æœç´¢å…³é”®è¯
            keywords = self._extract_search_keywords(news_item)
            
            # 2. æ‰§è¡Œç™¾åº¦æœç´¢
            search_results, search_success = self._perform_search(keywords, news_item.title)
            
            # 3. ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š
            deep_report = self._generate_deep_analysis(news_item, search_results, keywords)
            
            # 4. é‡æ–°è¯„ä¼°é‡è¦æ€§åˆ†æ•°
            adjusted_score = self._adjust_importance_score(
                news_item, deep_report, search_results
            ) if self.enable_score_adjustment else news_item.importance_score
            
            result = DeepAnalysisResult(
                news_id=news_item.id or f"deep_{int(time.time())}",
                title=news_item.title,
                original_score=news_item.importance_score,
                search_keywords=keywords,
                search_results_summary=search_results,
                deep_analysis_report=deep_report,
                adjusted_score=adjusted_score,
                analysis_time=datetime.now().isoformat(),
                search_success=search_success,
                model_used=self.config.get("ai_analysis", {}).get("openrouter", {}).get("model", "deepseek/deepseek-r1-0528:free")
            )
            
            logger.info(f"æ·±åº¦åˆ†æå®Œæˆ: {news_item.title[:50]}... -> {adjusted_score}åˆ† (åŸ{news_item.importance_score}åˆ†)")
            logger.info("æ·±åº¦åˆ†ææŠ¥å‘Šå…¨æ–‡:")
            logger.info(deep_report)
            return result
            
        except Exception as e:
            logger.error(f"æ·±åº¦åˆ†æå¤±è´¥: {e}")
            return self._create_error_result(news_item, str(e))
    
    def batch_analyze_deep(self, news_list: List[NewsItem]) -> List[DeepAnalysisResult]:
        """
        æ‰¹é‡æ·±åº¦åˆ†ææ–°é—»
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            List[DeepAnalysisResult]: æ·±åº¦åˆ†æç»“æœåˆ—è¡¨
        """
        # ç­›é€‰éœ€è¦æ·±åº¦åˆ†æçš„æ–°é—»
        high_importance_news = [news for news in news_list if self.should_analyze(news)]
        
        if not high_importance_news:
            logger.info("æ²¡æœ‰æ–°é—»éœ€è¦æ·±åº¦åˆ†æ")
            return []
        
        logger.info(f"å¼€å§‹æ‰¹é‡æ·±åº¦åˆ†æï¼Œå…± {len(high_importance_news)} æ¡é«˜é‡è¦æ€§æ–°é—»")
        
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘åˆ†æ
        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_news = {
                executor.submit(self.analyze_news_deep, news): news
                for news in high_importance_news
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_news):
                news = future_to_news[future]
                try:
                    result = future.result()
                    results.append(result)
                    logger.debug(f"æ·±åº¦åˆ†æå®Œæˆ: {news.title[:30]}...")
                except Exception as e:
                    logger.error(f"æ·±åº¦åˆ†æä»»åŠ¡å¤±è´¥: {news.title[:30]}... - {e}")
                    # æ·»åŠ é”™è¯¯ç»“æœ
                    error_result = self._create_error_result(news, str(e))
                    results.append(error_result)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        results.sort(key=lambda x: x.original_score, reverse=True)
        
        logger.info(f"æ‰¹é‡æ·±åº¦åˆ†æå®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡æ–°é—»")
        return results
    
    def _extract_search_keywords(self, news_item: NewsItem) -> List[str]:
        """
        ä»æ–°é—»ä¸­æå–æœç´¢å…³é”®è¯
        
        Args:
            news_item: æ–°é—»é¡¹
            
        Returns:
            List[str]: æœç´¢å…³é”®è¯åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ç®€å•çš„å…³é”®è¯æå–ç­–ç•¥
            title = news_item.title
            content = news_item.content
            
            keywords = []
            
            # ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
            title_keywords = self._extract_keywords_from_text(title)
            keywords.extend(title_keywords[:3])  # å–å‰3ä¸ª
            
            # ä»å†…å®¹ä¸­æå–å…³é”®è¯
            if content and len(keywords) < self.max_keywords:
                content_keywords = self._extract_keywords_from_text(content)
                remaining_slots = self.max_keywords - len(keywords)
                keywords.extend(content_keywords[:remaining_slots])
            
            # å¦‚æœå…³é”®è¯ä¸è¶³ï¼Œä½¿ç”¨æ ‡é¢˜ä½œä¸ºæœç´¢è¯
            if not keywords:
                keywords = [title[:20]]  # ä½¿ç”¨æ ‡é¢˜å‰20å­—ç¬¦
            
            logger.debug(f"æå–æœç´¢å…³é”®è¯: {keywords}")
            return keywords[:self.max_keywords]
            
        except Exception as e:
            logger.error(f"æå–æœç´¢å…³é”®è¯å¤±è´¥: {e}")
            return [news_item.title[:20]]  # é™çº§æ–¹æ¡ˆ
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        if not text:
            return []
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼šæŸ¥æ‰¾å¸¸è§çš„è´¢ç»å…³é”®è¯
        financial_keywords = [
            "è‚¡ç¥¨", "è‚¡å¸‚", "ä¸Šå¸‚", "IPO", "èèµ„", "æŠ•èµ„", "åŸºé‡‘", "è¯åˆ¸",
            "é“¶è¡Œ", "ä¿é™©", "åœ°äº§", "ç§‘æŠ€", "åŒ»è¯", "èƒ½æº", "æ±½è½¦", "æ¶ˆè´¹",
            "åˆ¶é€ ", "é‡‘è", "äº’è”ç½‘", "äººå·¥æ™ºèƒ½", "æ–°èƒ½æº", "åŠå¯¼ä½“",
            "æ¶¨åœ", "è·Œåœ", "æ¶¨å¹…", "è·Œå¹…", "æˆäº¤", "å¸‚å€¼", "ä¸šç»©", "è´¢æŠ¥"
        ]
        
        # æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
        found_keywords = []
        for keyword in financial_keywords:
            if keyword in text and keyword not in found_keywords:
                found_keywords.append(keyword)
                if len(found_keywords) >= 5:  # æœ€å¤šæå–5ä¸ª
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è´¢ç»å…³é”®è¯ï¼Œæå–å…¶ä»–å…³é”®è¯
        if not found_keywords:
            # ç®€å•åˆ†è¯ï¼šæå–3-8å­—ç¬¦çš„è¯ç»„
            import re
            words = re.findall(r'[\u4e00-\u9fff]{3,8}', text)
            found_keywords = list(set(words))[:3]
        
        return found_keywords
    
    def _perform_search(self, keywords: List[str], title: str) -> Tuple[str, bool]:
        """
        æ‰§è¡Œç™¾åº¦æœç´¢
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            title: æ–°é—»æ ‡é¢˜ï¼ˆä½œä¸ºå¤‡ç”¨æœç´¢è¯ï¼‰
            
        Returns:
            Tuple[str, bool]: (æœç´¢ç»“æœæ‘˜è¦, æœç´¢æ˜¯å¦æˆåŠŸ)
        """
        try:
            # ç»„åˆå…³é”®è¯è¿›è¡Œæœç´¢
            search_query = " ".join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
            
            if not search_query.strip():
                search_query = title[:30]  # ä½¿ç”¨æ ‡é¢˜å‰30å­—ç¬¦ä½œä¸ºå¤‡ç”¨
            
            logger.info(f"æ‰§è¡Œç™¾åº¦æœç´¢: {search_query}")
            
            # è°ƒç”¨ç™¾åº¦æœç´¢å·¥å…·
            search_result = baidu_search_tool(search_query, max_results=5)
            
            if search_result and "æœç´¢å¤±è´¥" not in search_result:
                logger.info(f"æœç´¢æˆåŠŸ: {search_query}")
                return search_result, True
            else:
                logger.warning(f"æœç´¢å¤±è´¥: {search_query}")
                return f"æœç´¢å…³é”®è¯'{search_query}'æœªè·å–åˆ°æœ‰æ•ˆç»“æœ", False
                
        except Exception as e:
            logger.error(f"æ‰§è¡Œæœç´¢æ—¶å‡ºé”™: {e}")
            return f"æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", False
    
    def _generate_deep_analysis(self, news_item: NewsItem, search_results: str, keywords: List[str]) -> str:
        """
        ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š
        
        Args:
            news_item: æ–°é—»é¡¹
            search_results: æœç´¢ç»“æœ
            keywords: æœç´¢å…³é”®è¯
            
        Returns:
            str: æ·±åº¦åˆ†ææŠ¥å‘Š
        """
        if self.client is None:
            return self._generate_mock_analysis(news_item, search_results, keywords)
        
        try:
            prompt = self._build_deep_analysis_prompt(news_item, search_results, keywords)
            response = self._call_ai_model(prompt)
            
            # è§£æå’Œæ¸…ç†åˆ†æç»“æœ
            analysis = self._parse_analysis_response(response)
            
            # ç¡®ä¿é•¿åº¦é™åˆ¶
            if len(analysis) > self.report_max_length:
                analysis = analysis[:self.report_max_length-3] + "..."
            
            return analysis
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return self._generate_mock_analysis(news_item, search_results, keywords)
    
    def _build_deep_analysis_prompt(self, news_item: NewsItem, search_results: str, keywords: List[str]) -> str:
        """æ„å»ºæ·±åº¦åˆ†æçš„prompt"""
        
        prompt = f"""ä½œä¸ºä¸“ä¸šçš„è´¢ç»åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œæ·±åº¦åˆ†æã€‚

åŸå§‹æ–°é—»ï¼š
æ ‡é¢˜ï¼š{news_item.title}
å†…å®¹ï¼š{news_item.content}
æ¥æºï¼š{news_item.source}
é‡è¦æ€§åˆ†æ•°ï¼š{news_item.importance_score}åˆ†

ç›¸å…³èƒŒæ™¯ä¿¡æ¯ï¼ˆé€šè¿‡æœç´¢å…³é”®è¯"{', '.join(keywords)}"è·å–ï¼‰ï¼š
{search_results}

è¯·åŸºäºåŸå§‹æ–°é—»å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½200å­—ä»¥å†…çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œé‡ç‚¹åˆ†æï¼š
1. æ–°é—»çš„æ·±å±‚å½±å“å’Œæ„ä¹‰
2. å¯¹ç›¸å…³è¡Œä¸šæˆ–å¸‚åœºçš„æ½œåœ¨å½±å“
3. å¯èƒ½çš„å‘å±•è¶‹åŠ¿
4. æŠ•èµ„è€…éœ€è¦å…³æ³¨çš„è¦ç‚¹

è¦æ±‚ï¼š
- ä¸“ä¸šã€å®¢è§‚ã€å‡†ç¡®
- æ§åˆ¶åœ¨200å­—ä»¥å†…
- é‡ç‚¹çªå‡ºï¼Œæ¡ç†æ¸…æ™°
- ç»“åˆèƒŒæ™¯ä¿¡æ¯æä¾›æ›´æ·±å±‚æ¬¡çš„æ´å¯Ÿ

æ·±åº¦åˆ†ææŠ¥å‘Šï¼š"""

        return prompt
    
    def _call_ai_model(self, prompt: str) -> str:
        """è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ"""
        try:
            openrouter_config = self.config.get("ai_analysis", {}).get("openrouter", {})
            model = openrouter_config.get("model", "deepseek/deepseek-r1-0528:free")
            
            # è¯»å–æ·±åº¦åˆ†æä¸“å±max_tokensï¼Œé»˜è®¤100000ï¼Œå¹¶åšå®‰å…¨è£å‰ª
            deep_max_tokens = self.deep_config.get("max_tokens", 100000)
            # OpenRouterå¾ˆå¤šæ¨¡å‹å­˜åœ¨ä¸Šä¸‹æ–‡é™åˆ¶ï¼Œè¿™é‡Œè®¾å®šç¡¬ä¸Šé™100000ï¼Œé¿å…è¯·æ±‚è¢«æ‹’
            safe_max_tokens = max(1, min(deep_max_tokens, 100000))

            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=safe_max_tokens,
                temperature=openrouter_config.get("temperature", 0.1)
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"è°ƒç”¨AIæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _parse_analysis_response(self, response: str) -> str:
        """è§£æAIåˆ†æå“åº”"""
        try:
            # ç®€å•æ¸…ç†å“åº”å†…å®¹
            analysis = response.strip()
            
            # ç§»é™¤å¯èƒ½çš„å‰ç¼€
            prefixes = ["æ·±åº¦åˆ†ææŠ¥å‘Šï¼š", "åˆ†ææŠ¥å‘Šï¼š", "æŠ¥å‘Šï¼š", "åˆ†æï¼š"]
            for prefix in prefixes:
                if analysis.startswith(prefix):
                    analysis = analysis[len(prefix):].strip()
                    break
            
            return analysis
            
        except Exception as e:
            logger.error(f"è§£æåˆ†æå“åº”å¤±è´¥: {e}")
            return response  # è¿”å›åŸå§‹å“åº”
    
    def _adjust_importance_score(self, news_item: NewsItem, deep_analysis: str, search_results: str) -> int:
        """
        æ ¹æ®æ·±åº¦åˆ†æè°ƒæ•´é‡è¦æ€§åˆ†æ•°
        
        Args:
            news_item: æ–°é—»é¡¹
            deep_analysis: æ·±åº¦åˆ†ææŠ¥å‘Š
            search_results: æœç´¢ç»“æœ
            
        Returns:
            int: è°ƒæ•´åçš„é‡è¦æ€§åˆ†æ•°
        """
        try:
            original_score = news_item.importance_score
            
            # ç®€å•çš„åˆ†æ•°è°ƒæ•´é€»è¾‘
            adjustment = 0
            
            # åŸºäºæ·±åº¦åˆ†æå†…å®¹çš„å…³é”®è¯è°ƒæ•´
            high_impact_keywords = ["é‡å¤§", "çªç ´", "é‡è¦", "å…³é”®", "æ˜¾è‘—", "å¤§å¹…", "æ€¥å‰§"]
            medium_impact_keywords = ["ä¸€å®š", "å¯èƒ½", "é¢„æœŸ", "æœ‰æœ›", "å½±å“"]
            
            analysis_text = deep_analysis.lower()
            
            for keyword in high_impact_keywords:
                if keyword in analysis_text:
                    adjustment += 2
            
            for keyword in medium_impact_keywords:
                if keyword in analysis_text:
                    adjustment += 1
            
            # åŸºäºæœç´¢ç»“æœæˆåŠŸä¸å¦è°ƒæ•´
            if "æœç´¢æˆåŠŸ" in search_results or len(search_results) > 100:
                adjustment += 3  # æœç´¢ç»“æœä¸°å¯Œï¼Œå¢åŠ å¯ä¿¡åº¦
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            adjusted_score = min(100, max(0, original_score + adjustment))
            
            logger.debug(f"åˆ†æ•°è°ƒæ•´: {original_score} -> {adjusted_score} (è°ƒæ•´å€¼: +{adjustment})")
            return adjusted_score
            
        except Exception as e:
            logger.error(f"è°ƒæ•´é‡è¦æ€§åˆ†æ•°å¤±è´¥: {e}")
            return news_item.importance_score  # è¿”å›åŸå§‹åˆ†æ•°
    
    def _generate_mock_analysis(self, news_item: NewsItem, search_results: str, keywords: List[str]) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ·±åº¦åˆ†ææŠ¥å‘Š"""
        
        has_search_results = search_results and "æœç´¢å¤±è´¥" not in search_results
        
        analysis = f"åŸºäºæ–°é—»'{news_item.title}'çš„æ·±åº¦åˆ†æï¼šè¯¥æ–°é—»æ¶‰åŠ{', '.join(keywords[:2])}ç­‰å…³é”®é¢†åŸŸã€‚"
        
        if has_search_results:
            analysis += "ç»“åˆç›¸å…³èƒŒæ™¯ä¿¡æ¯ï¼Œæ­¤äº‹ä»¶å¯èƒ½å¯¹ç›¸å…³è¡Œä¸šäº§ç”Ÿä¸€å®šå½±å“ã€‚"
        else:
            analysis += "ç”±äºèƒŒæ™¯ä¿¡æ¯æœ‰é™ï¼Œå»ºè®®æŒç»­å…³æ³¨åç»­å‘å±•ã€‚"
        
        analysis += "æŠ•èµ„è€…åº”å…³æ³¨ç›¸å…³æ”¿ç­–åŠ¨å‘å’Œå¸‚åœºååº”ï¼Œè°¨æ…è¯„ä¼°æŠ•èµ„é£é™©ã€‚"
        
        # ç¡®ä¿é•¿åº¦é™åˆ¶
        if len(analysis) > self.report_max_length:
            analysis = analysis[:self.report_max_length-3] + "..."
        
        return analysis
    
    def _create_skip_result(self, news_item: NewsItem) -> DeepAnalysisResult:
        """åˆ›å»ºè·³è¿‡åˆ†æçš„ç»“æœ"""
        return DeepAnalysisResult(
            news_id=news_item.id or f"skip_{int(time.time())}",
            title=news_item.title,
            original_score=getattr(news_item, 'importance_score', 0),
            search_keywords=[],
            search_results_summary="æœªè§¦å‘æ·±åº¦åˆ†ææ¡ä»¶",
            deep_analysis_report="è¯¥æ–°é—»é‡è¦æ€§åˆ†æ•°æœªè¾¾åˆ°æ·±åº¦åˆ†æé˜ˆå€¼",
            adjusted_score=getattr(news_item, 'importance_score', 0),
            analysis_time=datetime.now().isoformat(),
            search_success=False,
            model_used="skip"
        )
    
    def _create_error_result(self, news_item: NewsItem, error_msg: str) -> DeepAnalysisResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return DeepAnalysisResult(
            news_id=news_item.id or f"error_{int(time.time())}",
            title=news_item.title,
            original_score=getattr(news_item, 'importance_score', 0),
            search_keywords=[],
            search_results_summary=f"åˆ†æè¿‡ç¨‹å‡ºé”™: {error_msg}",
            deep_analysis_report="ç”±äºæŠ€æœ¯é—®é¢˜ï¼Œæ— æ³•å®Œæˆæ·±åº¦åˆ†æ",
            adjusted_score=getattr(news_item, 'importance_score', 0),
            analysis_time=datetime.now().isoformat(),
            search_success=False,
            model_used="error"
        )


if __name__ == '__main__':
    # æµ‹è¯•åŠŸèƒ½
    logger.info("ğŸ” æµ‹è¯•æ–°é—»æ·±åº¦åˆ†æå™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ–°é—»
    test_news = NewsItem(
        title="å¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹ï¼Œé‡Šæ”¾æµåŠ¨æ€§çº¦1ä¸‡äº¿å…ƒ",
        content="ä¸­å›½äººæ°‘é“¶è¡Œå†³å®šäº2024å¹´12æœˆ15æ—¥ä¸‹è°ƒé‡‘èæœºæ„å­˜æ¬¾å‡†å¤‡é‡‘ç‡0.5ä¸ªç™¾åˆ†ç‚¹ï¼Œæ­¤æ¬¡é™å‡†å°†é‡Šæ”¾é•¿æœŸèµ„é‡‘çº¦1ä¸‡äº¿å…ƒã€‚",
        source="å¤®è¡Œå®˜ç½‘",
        category="è´§å¸æ”¿ç­–"
    )
    test_news.importance_score = 85  # è®¾ç½®é«˜é‡è¦æ€§åˆ†æ•°
    
    analyzer = DeepAnalyzer()
    result = analyzer.analyze_news_deep(test_news)
    
    logger.info(f"ğŸ“° æ–°é—»: {result.title}")
    logger.info(f"ğŸ“Š åŸå§‹é‡è¦æ€§: {result.original_score} åˆ†")
    logger.info(f"ğŸ“Š è°ƒæ•´åé‡è¦æ€§: {result.adjusted_score} åˆ†")
    logger.info(f"ğŸ” æœç´¢å…³é”®è¯: {', '.join(result.search_keywords)}")
    logger.info(f"ğŸ” æœç´¢æˆåŠŸ: {result.search_success}")
    logger.info(f"ğŸ“ æ·±åº¦åˆ†æ: {result.deep_analysis_report}")
    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {result.model_used}") 