#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»æ·±åº¦åˆ†æå™¨
é’ˆå¯¹é«˜é‡è¦æ€§æ–°é—»ï¼ˆ>70åˆ†ï¼‰è¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. ç™¾åº¦æœç´¢è·å–èƒŒæ™¯ä¿¡æ¯
2. æ·±åº¦åˆ†ææŠ¥å‘Šç”Ÿæˆ
3. é‡è¦æ€§åˆ†æ•°é‡æ–°è¯„ä¼°
"""

import json
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Union, Tuple
import re

from openai import OpenAI
import yaml

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    from src.utils.database import NewsItem
    from src.utils.logger import get_logger
    from src.ai.ai_tools.baidu_search import baidu_search_tool
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å—ï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from ..utils.database import NewsItem
    from ..utils.logger import get_logger
    from ..ai_tools.baidu_search import baidu_search_tool

logger = get_logger("deep_analyzer")


@dataclass
class DeepAnalysisResult:
    """æ·±åº¦åˆ†æç»“æœ"""
    news_id: str
    title: str
    original_score: int           # åŸå§‹é‡è¦æ€§åˆ†æ•°
    search_keywords: List[str]    # æœç´¢å…³é”®è¯
    search_results_summary: str   # æœç´¢ç»“æœæ‘˜è¦
    deep_analysis_report: str     # 200å­—æ·±åº¦åˆ†ææŠ¥å‘Š
    adjusted_score: int           # è°ƒæ•´åçš„é‡è¦æ€§åˆ†æ•°
    analysis_time: str
    search_success: bool          # æœç´¢æ˜¯å¦æˆåŠŸ
    model_used: str              # ä½¿ç”¨çš„AIæ¨¡å‹


class DeepAnalyzer:
    """æ–°é—»æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ–æ·±åº¦åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or self._load_config()
        self.client = None
        self._init_client()
        
        # æ·±åº¦åˆ†æé…ç½®
        self.deep_config = self.config.get("ai_analysis", {}).get("deep_analysis", {})
        self.enabled = self.deep_config.get("enabled", True)
        self.score_threshold = self.deep_config.get("score_threshold", 70)
        self.max_concurrent = self.deep_config.get("max_concurrent", 3)
        self.search_timeout = self.deep_config.get("search_timeout", 30)
        self.max_keywords = self.deep_config.get("max_search_keywords", 5)
        self.report_max_length = self.deep_config.get("report_max_length", 200)
        self.enable_score_adjustment = self.deep_config.get("enable_score_adjustment", True)
        self.search_retry_count = self.deep_config.get("search_retry_count", 2)
        
        # AIè‡ªé©±åŠ¨æ£€ç´¢é…ç½®
        self.max_search_rounds = self.deep_config.get("max_search_rounds", 3)
        self.evidence_threshold = self.deep_config.get("evidence_threshold", 2)  # è‡³å°‘éœ€è¦çš„æœ‰æ•ˆè¯æ®æ•°
        self.max_evidence_kept = self.deep_config.get("max_evidence_kept", 5)   # ä¿ç•™çš„æœ€å¤§è¯æ®æ•°
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = os.path.join(
            os.path.dirname(__file__), "../../config/config.yaml"
        )
        
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯"""
        try:
            openrouter_config = self.config.get("ai_analysis", {}).get("openrouter", {})
            
            if not openrouter_config.get("api_key"):
                error_msg = "æœªé…ç½®OpenRouter APIå¯†é’¥ï¼Œæ·±åº¦åˆ†ææ— æ³•æ­£å¸¸å·¥ä½œ"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            self.client = OpenAI(
                api_key=openrouter_config.get("api_key"),
                base_url=openrouter_config.get("base_url", "https://openrouter.ai/api/v1")
            )
            
            logger.info("OpenRouteræ·±åº¦åˆ†æAPIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            error_msg = f"åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯å¤±è´¥: {e}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
    
    def should_analyze(self, news_item: NewsItem) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ·±åº¦åˆ†æ
        
        Args:
            news_item: æ–°é—»é¡¹
            
        Returns:
            bool: æ˜¯å¦éœ€è¦æ·±åº¦åˆ†æ
        """
        if not self.enabled:
            return False
            
        if not hasattr(news_item, 'importance_score') or news_item.importance_score is None:
            return False
            
        return news_item.importance_score >= self.score_threshold
    
    def analyze_news_deep(self, news_item: NewsItem) -> DeepAnalysisResult:
        """
        å¯¹å•æ¡æ–°é—»è¿›è¡Œæ·±åº¦åˆ†æ
        
        Args:
            news_item: æ–°é—»é¡¹
            
        Returns:
            DeepAnalysisResult: æ·±åº¦åˆ†æç»“æœ
        """
        if not self.should_analyze(news_item):
            logger.debug(f"æ–°é—»ä¸æ»¡è¶³æ·±åº¦åˆ†ææ¡ä»¶: {news_item.title}")
            return self._create_skip_result(news_item)
        
        try:
            logger.info(f"å¼€å§‹æ·±åº¦åˆ†æ: {news_item.title[:50]}...")
            
            # ä½¿ç”¨AIè‡ªé©±åŠ¨æ£€ç´¢æ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æ
            if self.client is not None:
                logger.info("ä½¿ç”¨AIè‡ªé©±åŠ¨æ£€ç´¢æ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æ")
                return self._analyze_with_ai_self_search(news_item)
            else:
                raise RuntimeError("OpenRouterå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œæ·±åº¦åˆ†æ")
                
        except Exception as e:
            logger.error(f"æ·±åº¦åˆ†æå¤±è´¥: {e}")
            return self._create_error_result(news_item, str(e))
    
    def _analyze_with_ai_self_search(self, news_item: NewsItem) -> DeepAnalysisResult:
        """
        ä½¿ç”¨AIè‡ªé©±åŠ¨æ£€ç´¢æ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æ
        
        Args:
            news_item: æ–°é—»é¡¹
            
        Returns:
            DeepAnalysisResult: æ·±åº¦åˆ†æç»“æœ
        """
        try:
            # 1. AIæŸ¥è¯¢è§„åˆ’ï¼šç”Ÿæˆæ£€ç´¢é—®é¢˜åˆ—è¡¨
            search_queries = self._generate_search_queries(news_item)
            logger.info(f"AIç”Ÿæˆæ£€ç´¢æŸ¥è¯¢: {search_queries}")
            
            # 2. å¤šè½®æœç´¢ï¼šæ‰§è¡Œæ¯ä¸ªæŸ¥è¯¢
            all_search_results = []
            successful_searches = 0
            
            for i, query in enumerate(search_queries):
                logger.info(f"æ‰§è¡Œç¬¬{i+1}è½®æœç´¢: {query}")
                search_result, success = self._perform_single_search(query)
                
                if success:
                    all_search_results.append({
                        'query': query,
                        'result': search_result,
                        'round': i + 1
                    })
                    successful_searches += 1
                    logger.info(f"ç¬¬{i+1}è½®æœç´¢æˆåŠŸï¼Œè·å¾—{len(search_result)}å­—ç¬¦ç»“æœ")
                else:
                    logger.warning(f"ç¬¬{i+1}è½®æœç´¢å¤±è´¥: {query}")
                
                # æå‰åœæ­¢æ¡ä»¶ï¼šè·å¾—è¶³å¤Ÿè¯æ®
                if successful_searches >= self.evidence_threshold:
                    logger.info(f"å·²è·å¾—{successful_searches}ä¸ªæœ‰æ•ˆæœç´¢ç»“æœï¼Œæ»¡è¶³è¯æ®é˜ˆå€¼")
                    break
            
            # 3. è¯æ®æ±‡æ€»ä¸è¯„åˆ†
            evidence_summary = self._evaluate_and_summarize_evidence(all_search_results, news_item)
            logger.info(f"è¯æ®æ±‡æ€»å®Œæˆï¼Œä¿ç•™{len(evidence_summary.get('top_evidence', []))}ä¸ªé«˜è´¨é‡è¯æ®")
            
            # 4. åŸºäºè¯æ®ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š
            deep_report = self._generate_evidence_based_analysis(news_item, evidence_summary)
            
            # 5. é‡æ–°è¯„ä¼°é‡è¦æ€§åˆ†æ•°
            adjusted_score = self._adjust_score_with_evidence(
                news_item, deep_report, evidence_summary
            ) if self.enable_score_adjustment else news_item.importance_score
            
            # æ„å»ºç»“æœ
            result = DeepAnalysisResult(
                news_id=news_item.id or f"ai_deep_{int(time.time())}",
                title=news_item.title,
                original_score=news_item.importance_score,
                search_keywords=[result['query'] for result in all_search_results],
                search_results_summary=evidence_summary.get('summary', ''),
                deep_analysis_report=deep_report,
                adjusted_score=adjusted_score,
                analysis_time=datetime.now().isoformat(),
                search_success=successful_searches > 0,
                model_used=self.config.get("ai_analysis", {}).get("openrouter", {}).get("model", "deepseek/deepseek-chat-v3.1")
            )
            
            logger.info(f"AIè‡ªé©±åŠ¨æ·±åº¦åˆ†æå®Œæˆ: {news_item.title[:50]}... -> {adjusted_score}åˆ† (åŸ{news_item.importance_score}åˆ†)")
            logger.info("æ·±åº¦åˆ†ææŠ¥å‘Šå…¨æ–‡:")
            logger.info(deep_report)
            return result
            
        except Exception as e:
            logger.error(f"AIè‡ªé©±åŠ¨æ·±åº¦åˆ†æå¤±è´¥: {e}")
            raise RuntimeError(f"AIè‡ªé©±åŠ¨æ·±åº¦åˆ†æå¤±è´¥: {e}")
    

    
    def batch_analyze_deep(self, news_list: List[NewsItem]) -> List[DeepAnalysisResult]:
        """
        æ‰¹é‡æ·±åº¦åˆ†ææ–°é—»
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            List[DeepAnalysisResult]: æ·±åº¦åˆ†æç»“æœåˆ—è¡¨
        """
        # ç­›é€‰éœ€è¦æ·±åº¦åˆ†æçš„æ–°é—»
        high_importance_news = [news for news in news_list if self.should_analyze(news)]
        
        if not high_importance_news:
            logger.info("æ²¡æœ‰æ–°é—»éœ€è¦æ·±åº¦åˆ†æ")
            return []
        
        logger.info(f"å¼€å§‹æ‰¹é‡æ·±åº¦åˆ†æï¼Œå…± {len(high_importance_news)} æ¡é«˜é‡è¦æ€§æ–°é—»")
        
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘åˆ†æ
        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_news = {
                executor.submit(self.analyze_news_deep, news): news
                for news in high_importance_news
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_news):
                news = future_to_news[future]
                try:
                    result = future.result()
                    results.append(result)
                    logger.debug(f"æ·±åº¦åˆ†æå®Œæˆ: {news.title[:30]}...")
                except Exception as e:
                    logger.error(f"æ·±åº¦åˆ†æä»»åŠ¡å¤±è´¥: {news.title[:30]}... - {e}")
                    # æ·»åŠ é”™è¯¯ç»“æœ
                    error_result = self._create_error_result(news, str(e))
                    results.append(error_result)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        results.sort(key=lambda x: x.original_score, reverse=True)
        
        logger.info(f"æ‰¹é‡æ·±åº¦åˆ†æå®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡æ–°é—»")
        return results
    
    def _generate_search_queries(self, news_item: NewsItem) -> List[str]:
        """
        AIç”Ÿæˆæ£€ç´¢æŸ¥è¯¢åˆ—è¡¨
        
        Args:
            news_item: æ–°é—»é¡¹
            
        Returns:
            List[str]: æ£€ç´¢æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            prompt = f"""ä½œä¸ºä¸“ä¸šçš„è´¢ç»ä¿¡æ¯åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹æ–°é—»ç”Ÿæˆ1-3ä¸ªæœ€æœ‰ä»·å€¼çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºè·å–ç›¸å…³èƒŒæ™¯ä¿¡æ¯å’Œæ·±åº¦åˆ†æç´ æã€‚

æ–°é—»ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{news_item.title}
å†…å®¹ï¼š{news_item.content}
æ¥æºï¼š{news_item.source}
é‡è¦æ€§åˆ†æ•°ï¼š{news_item.importance_score}åˆ†

è¯·ç”ŸæˆæŸ¥è¯¢æ—¶è€ƒè™‘ï¼š
1.å…ˆæŸ¥è¯¢åŸæ–°é—»ï¼Œå†æŸ¥è¯¢ç›¸å…³æ–°é—»
2.è€ƒè™‘ç›¸å…³å…¬å¸ã€è¡Œä¸šã€æ”¿ç­–

è¦æ±‚ï¼š
- æ¯ä¸ªæŸ¥è¯¢15-30å­—ï¼Œç²¾ç¡®æœ‰é’ˆå¯¹æ€§
- é¿å…è¿‡äºå®½æ³›çš„æœç´¢è¯
- ä¼˜å…ˆè·å–æƒå¨ã€æ—¶æ•ˆæ€§å¼ºçš„ä¿¡æ¯
- æŸ¥è¯¢åº”äº’è¡¥ï¼Œè¦†ç›–ä¸åŒç»´åº¦

è¯·ç›´æ¥è¾“å‡ºæŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œæ ¼å¼å¦‚ï¼š
1. æŸ¥è¯¢å†…å®¹1
2. æŸ¥è¯¢å†…å®¹2
3. æŸ¥è¯¢å†…å®¹3"""

            response = self._call_ai_model(prompt)
            
            # è§£ææŸ¥è¯¢åˆ—è¡¨
            queries = self._parse_search_queries(response)
            
            # é™åˆ¶æŸ¥è¯¢æ•°é‡
            queries = queries[:self.max_search_rounds]
            
            if not queries:
                # é™çº§æ–¹æ¡ˆï¼šåŸºäºæ–°é—»æ ‡é¢˜ç”ŸæˆæŸ¥è¯¢
                fallback_query = news_item.title[:25] + " æœ€æ–°æ¶ˆæ¯"
                queries = [fallback_query]
                logger.warning(f"AIæŸ¥è¯¢ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é™çº§æŸ¥è¯¢: {fallback_query}")
            
            return queries
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆ
            fallback_query = news_item.title[:25] + " ç›¸å…³ä¿¡æ¯"
            return [fallback_query]
    
    def _parse_search_queries(self, response: str) -> List[str]:
        """
        è§£æAIç”Ÿæˆçš„æœç´¢æŸ¥è¯¢å“åº”
        
        Args:
            response: AIå“åº”æ–‡æœ¬
            
        Returns:
            List[str]: è§£æå‡ºçš„æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            queries = []
            lines = response.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                # åŒ¹é…ç¼–å·æ ¼å¼ï¼š1. ã€2. ã€3. ç­‰
                if re.match(r'^\d+\.?\s*', line):
                    query = re.sub(r'^\d+\.?\s*', '', line).strip()
                    if query and len(query) >= 5:  # è¿‡æ»¤è¿‡çŸ­çš„æŸ¥è¯¢
                        queries.append(query)
                elif line and not line.startswith(('è¦æ±‚', 'è¯·', 'æ³¨æ„', 'æ ¼å¼')):
                    # æ²¡æœ‰ç¼–å·ä½†æ˜¯æœ‰å†…å®¹çš„è¡Œ
                    if len(line) >= 5:
                        queries.append(line)
            
            return queries[:self.max_search_rounds]
            
        except Exception as e:
            logger.error(f"è§£ææœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    

    
    def _perform_single_search(self, query: str) -> Tuple[str, bool]:
        """
        æ‰§è¡Œå•æ¬¡æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            Tuple[str, bool]: (æœç´¢ç»“æœ, æœç´¢æ˜¯å¦æˆåŠŸ)
        """
        try:
            logger.debug(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢: {query}")
            
            # è°ƒç”¨ç™¾åº¦æœç´¢å·¥å…·
            search_result = baidu_search_tool(query, max_results=3)
            
            if search_result and "æœç´¢å¤±è´¥" not in search_result and len(search_result) > 50:
                return search_result, True
            else:
                logger.warning(f"æœç´¢æŸ¥è¯¢æ— æ•ˆç»“æœ: {query}")
                return f"æŸ¥è¯¢'{query}'æœªè·å–åˆ°æœ‰æ•ˆç»“æœ", False
                
        except Exception as e:
            logger.error(f"æœç´¢æŸ¥è¯¢å‡ºé”™: {query} - {e}")
            return f"æœç´¢è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}", False
    
    def _evaluate_and_summarize_evidence(self, search_results: List[Dict], news_item: NewsItem) -> Dict:
        """
        è¯„ä¼°å’Œæ±‡æ€»æœç´¢è¯æ®
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« query, result, round
            news_item: åŸå§‹æ–°é—»é¡¹
            
        Returns:
            Dict: åŒ…å«è¯„åˆ†ã€æ±‡æ€»ç­‰ä¿¡æ¯çš„è¯æ®å­—å…¸
        """
        try:
            if not search_results:
                return {
                    'summary': 'æœªè·å–åˆ°æœ‰æ•ˆæœç´¢ç»“æœ',
                    'top_evidence': [],
                    'evidence_count': 0,
                    'avg_score': 0
                }
            
            # ä¸ºæ¯ä¸ªæœç´¢ç»“æœè¯„åˆ†
            scored_evidence = []
            
            for search_data in search_results:
                query = search_data['query']
                result = search_data['result']
                round_num = search_data['round']
                
                # è®¡ç®—è¯æ®è´¨é‡åˆ†æ•°
                score = self._calculate_evidence_score(result, query, news_item)
                
                scored_evidence.append({
                    'query': query,
                    'result': result,
                    'round': round_num,
                    'score': score,
                    'length': len(result)
                })
                
                logger.debug(f"ç¬¬{round_num}è½®æœç´¢è¯æ®è¯„åˆ†: {score:.2f} - {query[:20]}...")
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰Nä¸ª
            scored_evidence.sort(key=lambda x: x['score'], reverse=True)
            top_evidence = scored_evidence[:self.max_evidence_kept]
            
            # ç”Ÿæˆæ±‡æ€»
            evidence_summary = self._create_evidence_summary(top_evidence, news_item)
            
            avg_score = sum(e['score'] for e in scored_evidence) / len(scored_evidence) if scored_evidence else 0
            
            return {
                'summary': evidence_summary,
                'top_evidence': top_evidence,
                'evidence_count': len(search_results),
                'avg_score': avg_score,
                'total_length': sum(e['length'] for e in scored_evidence)
            }
            
        except Exception as e:
            logger.error(f"è¯æ®è¯„ä¼°ä¸æ±‡æ€»å¤±è´¥: {e}")
            return {
                'summary': f'è¯æ®å¤„ç†è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}',
                'top_evidence': [],
                'evidence_count': 0,
                'avg_score': 0
            }
    
    def _calculate_evidence_score(self, result: str, query: str, news_item: NewsItem) -> float:
        """
        è®¡ç®—å•ä¸ªè¯æ®çš„è´¨é‡åˆ†æ•°
        
        Args:
            result: æœç´¢ç»“æœæ–‡æœ¬
            query: æœç´¢æŸ¥è¯¢
            news_item: åŸå§‹æ–°é—»
            
        Returns:
            float: è¯æ®è´¨é‡åˆ†æ•° (0-10)
        """
        try:
            score = 0.0
            
            # 1. æƒå¨åº¦è¯„åˆ† (0-3åˆ†)
            authority_keywords = ['å®˜æ–¹', 'æ”¿åºœ', 'å¤®è¡Œ', 'è¯ç›‘ä¼š', 'é“¶ä¿ç›‘ä¼š', 'å‘æ”¹å§”', 'è´¢æ”¿éƒ¨', 'å•†åŠ¡éƒ¨', 'æ–°åç¤¾', 'äººæ°‘æ—¥æŠ¥']
            authority_score = 0
            for keyword in authority_keywords:
                if keyword in result:
                    authority_score += 0.5
                    if authority_score >= 3:
                        break
            score += min(authority_score, 3)
            
            # 2. ç›¸å…³åº¦è¯„åˆ† (0-2åˆ†)
            # æ£€æŸ¥æ–°é—»æ ‡é¢˜å…³é”®è¯åœ¨ç»“æœä¸­çš„å‡ºç°
            title_words = [word for word in news_item.title if len(word) >= 2]
            relevance_score = 0
            for word in title_words[:5]:  # å–å‰5ä¸ªè¯
                if word in result:
                    relevance_score += 0.4
            score += min(relevance_score, 2)
            
            # 3. ä¿¡æ¯å¯†åº¦è¯„åˆ† (0-2åˆ†)
            info_keywords = ['æ•°æ®', 'ç»Ÿè®¡', 'æŠ¥å‘Š', 'åˆ†æ', 'é¢„æµ‹', 'å½±å“', 'æ”¿ç­–', 'æªæ–½', 'æ–¹æ¡ˆ']
            info_score = 0
            for keyword in info_keywords:
                if keyword in result:
                    info_score += 0.3
            score += min(info_score, 2)
            
            # 4. æ—¶æ•ˆæ€§è¯„åˆ† (0-2åˆ†)
            time_keywords = ['æœ€æ–°', 'ä»Šæ—¥', 'åˆšåˆš', 'ä»Šå¹´', 'è¿‘æœŸ', 'ç›®å‰', 'ç°åœ¨', '2024', '2025']
            time_score = 0
            for keyword in time_keywords:
                if keyword in result:
                    time_score += 0.4
            score += min(time_score, 2)
            
            # 5. é•¿åº¦åˆç†æ€§ (0-1åˆ†)
            length = len(result)
            if 100 <= length <= 2000:
                length_score = 1.0
            elif 50 <= length < 100 or 2000 < length <= 5000:
                length_score = 0.5
            else:
                length_score = 0.1
            score += length_score
            
            return min(score, 10.0)
            
        except Exception as e:
            logger.error(f"è®¡ç®—è¯æ®åˆ†æ•°å¤±è´¥: {e}")
            return 0.0
    
    def _create_evidence_summary(self, top_evidence: List[Dict], news_item: NewsItem) -> str:
        """
        åˆ›å»ºè¯æ®æ±‡æ€»
        
        Args:
            top_evidence: æ’åºåçš„é¡¶çº§è¯æ®åˆ—è¡¨
            news_item: åŸå§‹æ–°é—»
            
        Returns:
            str: è¯æ®æ±‡æ€»æ–‡æœ¬
        """
        try:
            if not top_evidence:
                return "æœªè·å–åˆ°æœ‰æ•ˆè¯æ®"
            
            summary_parts = []
            
            for i, evidence in enumerate(top_evidence):
                query = evidence['query']
                result = evidence['result']
                score = evidence['score']
                
                # æˆªå–ç»“æœçš„å…³é”®éƒ¨åˆ†
                result_excerpt = result[:200] + "..." if len(result) > 200 else result
                
                summary_parts.append(f"è¯æ®{i+1}[æŸ¥è¯¢: {query}][è¯„åˆ†: {score:.1f}]: {result_excerpt}")
            
            return "\n\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºè¯æ®æ±‡æ€»å¤±è´¥: {e}")
            return "è¯æ®æ±‡æ€»å¤„ç†å‡ºé”™"
    
    def _generate_evidence_based_analysis(self, news_item: NewsItem, evidence_summary: Dict) -> str:
        """
        åŸºäºè¯æ®ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š
        
        Args:
            news_item: æ–°é—»é¡¹
            evidence_summary: è¯æ®æ±‡æ€»å­—å…¸
            
        Returns:
            str: æ·±åº¦åˆ†ææŠ¥å‘Š
        """
        try:
            prompt = self._build_evidence_based_analysis_prompt(news_item, evidence_summary)
            response = self._call_ai_model(prompt)
            
            # è§£æå’Œæ¸…ç†åˆ†æç»“æœ
            analysis = self._parse_analysis_response(response)
            
            # ç¡®ä¿é•¿åº¦é™åˆ¶
            if len(analysis) > self.report_max_length:
                analysis = analysis[:self.report_max_length-3] + "..."
            
            return analysis
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯æ®åŸºäºæ·±åº¦åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            raise RuntimeError(f"ç”Ÿæˆè¯æ®åŸºäºæ·±åº¦åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
    
    def _build_evidence_based_analysis_prompt(self, news_item: NewsItem, evidence_summary: Dict) -> str:
        """æ„å»ºåŸºäºè¯æ®ç”Ÿæˆæ·±åº¦åˆ†æçš„prompt"""
        
        prompt = f"""ä½œä¸ºä¸“ä¸šçš„è´¢ç»åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹è¯æ®å’Œæ–°é—»ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½200å­—ä»¥å†…çš„æ·±åº¦åˆ†ææŠ¥å‘Šã€‚

åŸå§‹æ–°é—»ï¼š
æ ‡é¢˜ï¼š{news_item.title}
å†…å®¹ï¼š{news_item.content}
æ¥æºï¼š{news_item.source}
é‡è¦æ€§åˆ†æ•°ï¼š{news_item.importance_score}åˆ†

è¯æ®æ±‡æ€»ï¼š
{evidence_summary.get('summary', 'æœªè·å–åˆ°æœ‰æ•ˆè¯æ®')}

è¯·åŸºäºåŸå§‹æ–°é—»å’Œè¯æ®ï¼Œç”Ÿæˆä¸€ä»½200å­—ä»¥å†…çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œé‡ç‚¹åˆ†æï¼š
1. æ–°é—»çš„æ·±å±‚å½±å“å’Œæ„ä¹‰
2. å¯¹ç›¸å…³è¡Œä¸šæˆ–å¸‚åœºçš„æ½œåœ¨å½±å“
3. å¯èƒ½çš„å‘å±•è¶‹åŠ¿
4. æŠ•èµ„è€…éœ€è¦å…³æ³¨çš„è¦ç‚¹

è¦æ±‚ï¼š
- ä¸“ä¸šã€å®¢è§‚ã€å‡†ç¡®
- æ§åˆ¶åœ¨200å­—ä»¥å†…
- é‡ç‚¹çªå‡ºï¼Œæ¡ç†æ¸…æ™°
- ç»“åˆè¯æ®æä¾›æ›´æ·±å±‚æ¬¡çš„æ´å¯Ÿ

æ·±åº¦åˆ†ææŠ¥å‘Šï¼š"""

        return prompt
    

    

    
    def _call_ai_model(self, prompt: str) -> str:
        """è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ"""
        try:
            openrouter_config = self.config.get("ai_analysis", {}).get("openrouter", {})
            model = openrouter_config.get("model", "deepseek/deepseek-chat-v3.1")
            
            # è¯»å–æ·±åº¦åˆ†æä¸“å±max_tokensï¼Œé»˜è®¤100000ï¼Œå¹¶åšå®‰å…¨è£å‰ª
            deep_max_tokens = self.deep_config.get("max_tokens", 100000)
            # OpenRouterå¾ˆå¤šæ¨¡å‹å­˜åœ¨ä¸Šä¸‹æ–‡é™åˆ¶ï¼Œè¿™é‡Œè®¾å®šç¡¬ä¸Šé™100000ï¼Œé¿å…è¯·æ±‚è¢«æ‹’
            safe_max_tokens = max(1, min(deep_max_tokens, 100000))

            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=safe_max_tokens,
                temperature=openrouter_config.get("temperature", 0.1)
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"è°ƒç”¨AIæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _parse_analysis_response(self, response: str) -> str:
        """è§£æAIåˆ†æå“åº”"""
        try:
            # ç®€å•æ¸…ç†å“åº”å†…å®¹
            analysis = response.strip()
            
            # ç§»é™¤å¯èƒ½çš„å‰ç¼€
            prefixes = ["æ·±åº¦åˆ†ææŠ¥å‘Šï¼š", "åˆ†ææŠ¥å‘Šï¼š", "æŠ¥å‘Šï¼š", "åˆ†æï¼š"]
            for prefix in prefixes:
                if analysis.startswith(prefix):
                    analysis = analysis[len(prefix):].strip()
                    break
            
            return analysis
            
        except Exception as e:
            logger.error(f"è§£æåˆ†æå“åº”å¤±è´¥: {e}")
            return response  # è¿”å›åŸå§‹å“åº”
    

    

    
    def _create_skip_result(self, news_item: NewsItem) -> DeepAnalysisResult:
        """åˆ›å»ºè·³è¿‡åˆ†æçš„ç»“æœ"""
        return DeepAnalysisResult(
            news_id=news_item.id or f"skip_{int(time.time())}",
            title=news_item.title,
            original_score=getattr(news_item, 'importance_score', 0),
            search_keywords=[],
            search_results_summary="æœªè§¦å‘æ·±åº¦åˆ†ææ¡ä»¶",
            deep_analysis_report="è¯¥æ–°é—»é‡è¦æ€§åˆ†æ•°æœªè¾¾åˆ°æ·±åº¦åˆ†æé˜ˆå€¼",
            adjusted_score=getattr(news_item, 'importance_score', 0),
            analysis_time=datetime.now().isoformat(),
            search_success=False,
            model_used="skip"
        )
    
    def _create_error_result(self, news_item: NewsItem, error_msg: str) -> DeepAnalysisResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return DeepAnalysisResult(
            news_id=news_item.id or f"error_{int(time.time())}",
            title=news_item.title,
            original_score=getattr(news_item, 'importance_score', 0),
            search_keywords=[],
            search_results_summary=f"åˆ†æè¿‡ç¨‹å‡ºé”™: {error_msg}",
            deep_analysis_report="ç”±äºæŠ€æœ¯é—®é¢˜ï¼Œæ— æ³•å®Œæˆæ·±åº¦åˆ†æ",
            adjusted_score=getattr(news_item, 'importance_score', 0),
            analysis_time=datetime.now().isoformat(),
            search_success=False,
            model_used="error"
        )
    
    def _adjust_score_with_evidence(self, news_item: NewsItem, deep_analysis: str, evidence_summary: Dict) -> int:
        """
        åŸºäºè¯æ®å’Œæ·±åº¦åˆ†æè°ƒæ•´é‡è¦æ€§åˆ†æ•°
        
        Args:
            news_item: æ–°é—»é¡¹
            deep_analysis: æ·±åº¦åˆ†ææŠ¥å‘Š
            evidence_summary: è¯æ®æ±‡æ€»å­—å…¸
            
        Returns:
            int: è°ƒæ•´åçš„é‡è¦æ€§åˆ†æ•°
        """
        try:
            original_score = news_item.importance_score
            adjustment = 0
            
            # 1. åŸºäºè¯æ®è´¨é‡è°ƒæ•´ (0-15åˆ†)
            avg_evidence_score = evidence_summary.get('avg_score', 0)
            evidence_count = evidence_summary.get('evidence_count', 0)
            
            if avg_evidence_score >= 7.0:
                adjustment += 10  # é«˜è´¨é‡è¯æ®
            elif avg_evidence_score >= 5.0:
                adjustment += 6   # ä¸­ç­‰è´¨é‡è¯æ®
            elif avg_evidence_score >= 3.0:
                adjustment += 3   # ä½è´¨é‡è¯æ®
            
            # é¢å¤–å¥–åŠ±å¤šä¸ªè¯æ®æ¥æº
            if evidence_count >= 3:
                adjustment += 3
            elif evidence_count >= 2:
                adjustment += 2
            
            # 2. åŸºäºæ·±åº¦åˆ†æå†…å®¹è°ƒæ•´ (0-10åˆ†)
            analysis_text = deep_analysis.lower()
            
            # é«˜å½±å“å…³é”®è¯
            high_impact_keywords = ['é‡å¤§', 'çªç ´', 'å…³é”®', 'æ˜¾è‘—', 'æ€¥å‰§', 'æš´è·Œ', 'æš´æ¶¨', 'é‡è¦']
            high_impact_count = sum(1 for keyword in high_impact_keywords if keyword in analysis_text)
            adjustment += min(high_impact_count * 2, 6)
            
            # å¸‚åœºæ•æ„Ÿè¯
            market_keywords = ['æ”¿ç­–', 'åˆ©ç‡', 'æ±‡ç‡', 'å¤®è¡Œ', 'ç›‘ç®¡', 'æ”¹é©', 'é£é™©']
            market_count = sum(1 for keyword in market_keywords if keyword in analysis_text)
            adjustment += min(market_count * 1, 4)
            
            # 3. åŸºäºè¯æ®æƒå¨æ€§é¢å¤–è°ƒæ•´ (0-5åˆ†)
            evidence_text = evidence_summary.get('summary', '')
            authority_keywords = ['å¤®è¡Œ', 'é“¶ä¿ç›‘ä¼š', 'è¯ç›‘ä¼š', 'æ”¿åºœ', 'å®˜æ–¹', 'æ–°åç¤¾', 'äººæ°‘æ—¥æŠ¥']
            authority_count = sum(1 for keyword in authority_keywords if keyword in evidence_text)
            adjustment += min(authority_count * 1, 5)
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            adjusted_score = min(100, max(0, original_score + adjustment))
            
            logger.info(f"åŸºäºè¯æ®çš„åˆ†æ•°è°ƒæ•´: {original_score} -> {adjusted_score} (è°ƒæ•´å€¼: +{adjustment})")
            logger.info(f"è°ƒæ•´å› å­ - è¯æ®è´¨é‡: {avg_evidence_score:.1f}, è¯æ®æ•°é‡: {evidence_count}, æƒå¨æ€§: {authority_count}")
            
            return adjusted_score
            
        except Exception as e:
            logger.error(f"åŸºäºè¯æ®è°ƒæ•´é‡è¦æ€§åˆ†æ•°å¤±è´¥: {e}")
            return news_item.importance_score  # è¿”å›åŸå§‹åˆ†æ•°


if __name__ == '__main__':
    # æµ‹è¯•åŠŸèƒ½
    logger.info("ğŸ” æµ‹è¯•æ–°é—»æ·±åº¦åˆ†æå™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ–°é—»
    test_news = NewsItem(
        title="å¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹ï¼Œé‡Šæ”¾æµåŠ¨æ€§çº¦1ä¸‡äº¿å…ƒ",
        content="ä¸­å›½äººæ°‘é“¶è¡Œå†³å®šäº2024å¹´12æœˆ15æ—¥ä¸‹è°ƒé‡‘èæœºæ„å­˜æ¬¾å‡†å¤‡é‡‘ç‡0.5ä¸ªç™¾åˆ†ç‚¹ï¼Œæ­¤æ¬¡é™å‡†å°†é‡Šæ”¾é•¿æœŸèµ„é‡‘çº¦1ä¸‡äº¿å…ƒã€‚",
        source="å¤®è¡Œå®˜ç½‘",
        category="è´§å¸æ”¿ç­–"
    )
    test_news.importance_score = 85  # è®¾ç½®é«˜é‡è¦æ€§åˆ†æ•°
    
    analyzer = DeepAnalyzer()
    result = analyzer.analyze_news_deep(test_news)
    
    logger.info(f"ğŸ“° æ–°é—»: {result.title}")
    logger.info(f"ğŸ“Š åŸå§‹é‡è¦æ€§: {result.original_score} åˆ†")
    logger.info(f"ğŸ“Š è°ƒæ•´åé‡è¦æ€§: {result.adjusted_score} åˆ†")
    logger.info(f"ğŸ” æœç´¢å…³é”®è¯: {', '.join(result.search_keywords)}")
    logger.info(f"ğŸ” æœç´¢æˆåŠŸ: {result.search_success}")
    logger.info(f"ğŸ“ æ·±åº¦åˆ†æ: {result.deep_analysis_report}")
    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {result.model_used}") 