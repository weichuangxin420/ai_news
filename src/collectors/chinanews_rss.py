#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­å›½æ–°é—»ç½‘RSSæ”¶é›†å™¨

ä»ä¸­å›½æ–°é—»ç½‘RSSè®¢é˜…æºæ”¶é›†è´¢ç»æ–°é—»
RSSæºï¼šhttps://www.chinanews.com.cn/rss/finance.xml
"""

import feedparser
import requests
from datetime import datetime, timezone
from typing import List, Dict, Optional
import logging
from urllib.parse import urljoin
import time
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    from src.utils.logger import logger
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥é¡¹ç›®loggerï¼Œä½¿ç”¨æ ‡å‡†logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)


class ChinaNewsRSSCollector:
    """ä¸­å›½æ–°é—»ç½‘RSSè´¢ç»æ–°é—»æ”¶é›†å™¨"""
    
    def __init__(self):
        self.base_url = "https://www.chinanews.com.cn"
        self.rss_url = "https://www.chinanews.com.cn/rss/finance.xml"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        self.timeout = 30
        
    def fetch_news(self, max_items: int = 50) -> List[Dict]:
        """
        è·å–è´¢ç»æ–°é—»
        
        Args:
            max_items: æœ€å¤§è·å–æ¡æ•°
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹è·å–ä¸­å›½æ–°é—»ç½‘è´¢ç»RSS: {self.rss_url}")
            
            # è·å–RSSå†…å®¹
            response = self.session.get(
                self.rss_url, 
                timeout=self.timeout,
                verify=False  # æœ‰äº›ç½‘ç«™SSLè¯ä¹¦å¯èƒ½æœ‰é—®é¢˜
            )
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æRSS
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                logger.warning(f"RSSè§£æå¯èƒ½æœ‰é—®é¢˜: {feed.bozo_exception}")
            
            news_list = []
            total_entries = len(feed.entries)
            logger.info(f"RSSåŒ…å« {total_entries} æ¡æ–°é—»")
            
            for i, entry in enumerate(feed.entries[:max_items]):
                try:
                    news_item = self._parse_entry(entry)
                    if news_item:
                        news_list.append(news_item)
                        
                except Exception as e:
                    logger.error(f"è§£æç¬¬{i+1}æ¡æ–°é—»å¤±è´¥: {e}")
                    continue
                    
                # æ·»åŠ å°å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if i > 0 and i % 10 == 0:
                    time.sleep(0.1)
            
            logger.info(f"æˆåŠŸè·å– {len(news_list)} æ¡è´¢ç»æ–°é—»")
            return news_list
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–RSSå¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"è§£æRSSå¤±è´¥: {e}")
            return []
    
    def _parse_entry(self, entry) -> Optional[Dict]:
        """
        è§£æRSSæ¡ç›®
        
        Args:
            entry: RSSæ¡ç›®
            
        Returns:
            æ–°é—»æ•°æ®å­—å…¸
        """
        try:
            # æ ‡é¢˜
            title = entry.get('title', '').strip()
            if not title:
                return None
            
            # é“¾æ¥
            link = entry.get('link', '').strip()
            if not link:
                return None
            
            # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
            if link.startswith('/'):
                link = urljoin(self.base_url, link)
            
            # å‘å¸ƒæ—¶é—´
            published_time = self._parse_time(entry)
            
            # æ‘˜è¦/æè¿°
            summary = entry.get('summary', '') or entry.get('description', '')
            summary = self._clean_html(summary)
            
            # ä½œè€…
            author = entry.get('author', '') or entry.get('dc_creator', '')
            
            # åˆ†ç±»
            categories = []
            if hasattr(entry, 'tags'):
                categories = [tag.get('term', '') for tag in entry.tags if tag.get('term')]
            
            # æ„å»ºæ–°é—»é¡¹
            news_item = {
                'title': title,
                'url': link,
                'content': summary,
                'published_time': published_time,
                'source': 'ä¸­å›½æ–°é—»ç½‘',
                'category': 'è´¢ç»',
                'author': author,
                'tags': categories,
                'collected_time': datetime.now(timezone.utc).isoformat(),
                'source_type': 'rss',
                'source_url': self.rss_url
            }
            
            return news_item
            
        except Exception as e:
            logger.error(f"è§£æRSSæ¡ç›®å¤±è´¥: {e}")
            return None
    
    def _parse_time(self, entry) -> str:
        """
        è§£æå‘å¸ƒæ—¶é—´
        
        Args:
            entry: RSSæ¡ç›®
            
        Returns:
            ISOæ ¼å¼æ—¶é—´å­—ç¬¦ä¸²
        """
        # å°è¯•å¤šä¸ªæ—¶é—´å­—æ®µ
        time_fields = ['published', 'updated', 'created']
        
        for field in time_fields:
            if hasattr(entry, f'{field}_parsed') and getattr(entry, f'{field}_parsed'):
                try:
                    time_struct = getattr(entry, f'{field}_parsed')
                    dt = datetime(*time_struct[:6], tzinfo=timezone.utc)
                    return dt.isoformat()
                except (ValueError, TypeError):
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        return datetime.now(timezone.utc).isoformat()
    
    def _clean_html(self, text: str) -> str:
        """
        æ¸…ç†HTMLæ ‡ç­¾
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ''
        
        import re
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # è§£ç HTMLå®ä½“
        import html
        text = html.unescape(text)
        
        # æ¸…ç†å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def test_connection(self) -> Dict:
        """
        æµ‹è¯•RSSè¿æ¥
        
        Returns:
            æµ‹è¯•ç»“æœ
        """
        try:
            start_time = time.time()
            
            response = self.session.get(
                self.rss_url,
                timeout=self.timeout,
                verify=False
            )
            response.raise_for_status()
            
            elapsed_time = time.time() - start_time
            
            # ç®€å•è§£ææ£€æŸ¥
            feed = feedparser.parse(response.content)
            entry_count = len(feed.entries)
            
            result = {
                'status': 'success',
                'response_time': round(elapsed_time, 2),
                'status_code': response.status_code,
                'entry_count': entry_count,
                'feed_title': feed.feed.get('title', ''),
                'feed_description': feed.feed.get('description', ''),
                'last_updated': feed.feed.get('updated', ''),
                'error': None
            }
            
            logger.info(f"ä¸­å›½æ–°é—»ç½‘RSSè¿æ¥æµ‹è¯•æˆåŠŸ: {entry_count}æ¡æ–°é—»")
            return result
            
        except requests.exceptions.Timeout:
            error_msg = f"è¿æ¥è¶…æ—¶ (>{self.timeout}s)"
            logger.error(f"RSSè¿æ¥æµ‹è¯•å¤±è´¥: {error_msg}")
            return {
                'status': 'timeout',
                'response_time': self.timeout,
                'error': error_msg
            }
        except requests.exceptions.RequestException as e:
            error_msg = f"ç½‘ç»œé”™è¯¯: {e}"
            logger.error(f"RSSè¿æ¥æµ‹è¯•å¤±è´¥: {error_msg}")
            return {
                'status': 'error',
                'error': error_msg
            }
        except Exception as e:
            error_msg = f"è§£æé”™è¯¯: {e}"
            logger.error(f"RSSè¿æ¥æµ‹è¯•å¤±è´¥: {error_msg}")
            return {
                'status': 'error',
                'error': error_msg
            }


def main():
    """æµ‹è¯•å‡½æ•°"""
    collector = ChinaNewsRSSCollector()
    
    print("ğŸ” æµ‹è¯•ä¸­å›½æ–°é—»ç½‘è´¢ç»RSSè¿æ¥...")
    test_result = collector.test_connection()
    print(f"è¿æ¥çŠ¶æ€: {test_result['status']}")
    print(f"å“åº”æ—¶é—´: {test_result.get('response_time', 'N/A')}s")
    print(f"æ–°é—»æ¡æ•°: {test_result.get('entry_count', 'N/A')}")
    
    if test_result['status'] == 'success':
        print("\nğŸ“° è·å–æœ€æ–°è´¢ç»æ–°é—»...")
        news_list = collector.fetch_news(max_items=5)
        
        for i, news in enumerate(news_list, 1):
            print(f"\n--- æ–°é—» {i} ---")
            print(f"æ ‡é¢˜: {news['title']}")
            print(f"æ¥æº: {news['source']} - {news['category']}")
            print(f"æ—¶é—´: {news['published_time']}")
            print(f"é“¾æ¥: {news['url']}")
            print(f"æ‘˜è¦: {news['content'][:100]}...")
    else:
        print(f"âŒ è¿æ¥å¤±è´¥: {test_result.get('error', 'Unknown error')}")


if __name__ == '__main__':
    main() 